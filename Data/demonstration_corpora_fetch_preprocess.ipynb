{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7237b80a-d060-4440-9c66-f9c04137f14c",
   "metadata": {},
   "source": [
    "# Google BigQuery based data (text/code) scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0940cf4e-bb71-4fb9-8ffb-16eda6cfca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "# warnings.filterwarnings('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79914175-041c-4ad6-a0e5-d489fdefaf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbd6649d-e2c3-428e-bc34-e52cb72f415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your credentials to google cloud\n",
    "\n",
    "#https://developers.google.com/workspace/guides/create-credentials#google-cloud-console\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/Users/shailjathakur/Downloads/cellular-arbor-403001-8d6590ba7702.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c9a6b03-5096-4580-adae-11654bd1bab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bqclient = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66e58486-514a-4b77-9507-a97581896334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract some COBOL files from GitHub uisng Google BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a462ce9-887b-4c8a-8cce-4711add46094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_string=\"\"\"\n",
    "# SELECT f.repo_name, f.path, c.copies, c.size, c.content\n",
    "#  FROM `bigquery-public-data.github_repos.files` AS f \n",
    "#  JOIN `bigquery-public-data.github_repos.contents` AS c \n",
    "#  ON f.id = c.id \n",
    "#  WHERE \n",
    "#  NOT c.binary \n",
    "#  AND ((f.path LIKE '%.cbl') \n",
    "#  AND (c.size BETWEEN 1\n",
    "#  AND 500))\n",
    "# \"\"\"\n",
    "\n",
    "query_string=\"\"\"SELECT f.repo_name, c.content\n",
    "FROM `bigquery-public-data.github_repos.files` AS f\n",
    "JOIN `bigquery-public-data.github_repos.contents` AS c\n",
    "ON f.id = c.id\n",
    "WHERE\n",
    "NOT c.binary\n",
    "AND f.path LIKE '%.py'\n",
    "AND REGEXP_CONTAINS(c.content, r'(?m)^\\s*assert ')\n",
    "LIMIT 10\"\"\"\n",
    "\n",
    "\n",
    "# query = \"\"\"\n",
    "#     SELECT corpus AS title, COUNT(word) AS unique_words\n",
    "#     FROM `bigquery-public-data.samples.shakespeare`\n",
    "#     GROUP BY title\n",
    "#     ORDER BY unique_words\n",
    "#     DESC LIMIT 10\n",
    "# \"\"\"\n",
    "# results = bqclient.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa4544cd-585e-4e1d-8553-a14287a3ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(query_string):\n",
    "    \n",
    "    dataframe = (\n",
    "        bqclient.query(query_string)\n",
    "        .result()\n",
    "        .to_dataframe(\n",
    "            \n",
    "            create_bqstorage_client=True,\n",
    "        )\n",
    "    )\n",
    "    print(dataframe.head())\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4492ffc-5277-4ce9-935e-2787b624d900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 repo_name                                            content\n",
      "0  uglyboxer/linear_neuron  \"\"\"\\nProvides a collection of utilities for co...\n",
      "1           kstilwell/tcex  \"\"\"Test the TcEx Metrics Module.\"\"\"\\n# standar...\n",
      "2           kstilwell/tcex  \"\"\"Test the TcEx Threat Intel Module.\"\"\"\\n# st...\n",
      "3    nimbis/django-allauth  from __future__ import unicode_literals\\n\\nimp...\n",
      "4            dbs/schemaorg  # -*- coding: utf-8 -*-\\n#\\n\"\"\"\\nThis is an RD...\n"
     ]
    }
   ],
   "source": [
    "dataframe=query(query_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0aec190-9267-4a0d-8f18-3ceeb1ca2f29",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['*', '*', '*', '*', '*', '*', '*', '*', '*', '*']\n",
      "\"\"\"\n",
      "Provides a collection of utilities for comparing (image) results.\n",
      "\n",
      "\"\"\"\n",
      "from __future__ import (absolute_import, division, print_function,\n",
      "                        unicode_literals)\n",
      "\n",
      "import six\n",
      "\n",
      "import hashlib\n",
      "import os\n",
      "import shutil\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "import matplotlib\n",
      "from matplotlib.compat import subprocess\n",
      "from matplotlib.testing.noseclasses import ImageComparisonFailure\n",
      "from matplotlib import _png\n",
      "from matplotlib import _get_cachedir\n",
      "from matplotlib import cbook\n",
      "from distutils import version\n",
      "\n",
      "__all__ = ['compare_float', 'compare_images', 'comparable_formats']\n",
      "\n",
      "\n",
      "def make_test_filename(fname, purpose):\n",
      "    \"\"\"\n",
      "    Make a new filename by inserting `purpose` before the file's\n",
      "    extension.\n",
      "    \"\"\"\n",
      "    base, ext = os.path.splitext(fname)\n",
      "    return '%s-%s%s' % (base, purpose, ext)\n",
      "\n",
      "\n",
      "def compare_float(expected, actual, relTol=None, absTol=None):\n",
      "    \"\"\"\n",
      "    Fail if the floating point values are not close enough, with\n",
      "    the given message.\n",
      "\n",
      "    You can specify a relative tolerance, absolute tolerance, or both.\n",
      "\n",
      "    \"\"\"\n",
      "    if relTol is None and absTol is None:\n",
      "        raise ValueError(\"You haven't specified a 'relTol' relative \"\n",
      "                         \"tolerance or a 'absTol' absolute tolerance \"\n",
      "                         \"function argument. You must specify one.\")\n",
      "    msg = \"\"\n",
      "\n",
      "    if absTol is not None:\n",
      "        absDiff = abs(expected - actual)\n",
      "        if absTol < absDiff:\n",
      "            template = ['',\n",
      "                        'Expected: {expected}',\n",
      "                        'Actual:   {actual}',\n",
      "                        'Abs diff: {absDiff}',\n",
      "                        'Abs tol:  {absTol}']\n",
      "            msg += '\\n  '.join([line.format(**locals()) for line in template])\n",
      "\n",
      "    if relTol is not None:\n",
      "        # The relative difference of the two values.  If the expected value is\n",
      "        # zero, then return the absolute value of the difference.\n",
      "        relDiff = abs(expected - actual)\n",
      "        if expected:\n",
      "            relDiff = relDiff / abs(expected)\n",
      "\n",
      "        if relTol < relDiff:\n",
      "            # The relative difference is a ratio, so it's always unit-less.\n",
      "            template = ['',\n",
      "                        'Expected: {expected}',\n",
      "                        'Actual:   {actual}',\n",
      "                        'Rel diff: {relDiff}',\n",
      "                        'Rel tol:  {relTol}']\n",
      "            msg += '\\n  '.join([line.format(**locals()) for line in template])\n",
      "\n",
      "    return msg or None\n",
      "\n",
      "\n",
      "def get_cache_dir():\n",
      "    cachedir = _get_cachedir()\n",
      "    if cachedir is None:\n",
      "        raise RuntimeError('Could not find a suitable configuration directory')\n",
      "    cache_dir = os.path.join(cachedir, 'test_cache')\n",
      "    if not os.path.exists(cache_dir):\n",
      "        try:\n",
      "            cbook.mkdirs(cache_dir)\n",
      "        except IOError:\n",
      "            return None\n",
      "    if not os.access(cache_dir, os.W_OK):\n",
      "        return None\n",
      "    return cache_dir\n",
      "\n",
      "\n",
      "def get_file_hash(path, block_size=2 ** 20):\n",
      "    md5 = hashlib.md5()\n",
      "    with open(path, 'rb') as fd:\n",
      "        while True:\n",
      "            data = fd.read(block_size)\n",
      "            if not data:\n",
      "                break\n",
      "            md5.update(data)\n",
      "    return md5.hexdigest()\n",
      "\n",
      "\n",
      "def make_external_conversion_command(cmd):\n",
      "    def convert(old, new):\n",
      "        cmdline = cmd(old, new)\n",
      "        pipe = subprocess.Popen(\n",
      "            cmdline, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
      "        stdout, stderr = pipe.communicate()\n",
      "        errcode = pipe.wait()\n",
      "        if not os.path.exists(new) or errcode:\n",
      "            msg = \"Conversion command failed:\\n%s\\n\" % ' '.join(cmdline)\n",
      "            if stdout:\n",
      "                msg += \"Standard output:\\n%s\\n\" % stdout\n",
      "            if stderr:\n",
      "                msg += \"Standard error:\\n%s\\n\" % stderr\n",
      "            raise IOError(msg)\n",
      "\n",
      "    return convert\n",
      "\n",
      "\n",
      "def _update_converter():\n",
      "    gs, gs_v = matplotlib.checkdep_ghostscript()\n",
      "    if gs_v is not None:\n",
      "        cmd = lambda old, new: \\\n",
      "            [gs, '-q', '-sDEVICE=png16m', '-dNOPAUSE', '-dBATCH',\n",
      "             '-sOutputFile=' + new, old]\n",
      "        converter['pdf'] = make_external_conversion_command(cmd)\n",
      "        converter['eps'] = make_external_conversion_command(cmd)\n",
      "\n",
      "    if matplotlib.checkdep_inkscape() is not None:\n",
      "        cmd = lambda old, new: \\\n",
      "            ['inkscape', '-z', old, '--export-png', new]\n",
      "        converter['svg'] = make_external_conversion_command(cmd)\n",
      "\n",
      "\n",
      "#: A dictionary that maps filename extensions to functions which\n",
      "#: themselves map arguments `old` and `new` (filenames) to a list of strings.\n",
      "#: The list can then be passed to Popen to convert files with that\n",
      "#: extension to png format.\n",
      "converter = {}\n",
      "_update_converter()\n",
      "\n",
      "\n",
      "def comparable_formats():\n",
      "    \"\"\"\n",
      "    Returns the list of file formats that compare_images can compare\n",
      "    on this system.\n",
      "\n",
      "    \"\"\"\n",
      "    return ['png'] + list(six.iterkeys(converter))\n",
      "\n",
      "\n",
      "def convert(filename, cache):\n",
      "    \"\"\"\n",
      "    Convert the named file into a png file.  Returns the name of the\n",
      "    created file.\n",
      "\n",
      "    If *cache* is True, the result of the conversion is cached in\n",
      "    `matplotlib._get_cachedir() + '/test_cache/'`.  The caching is based\n",
      "    on a hash of the exact contents of the input file.  The is no limit\n",
      "    on the size of the cache, so it may need to be manually cleared\n",
      "    periodically.\n",
      "\n",
      "    \"\"\"\n",
      "    base, extension = filename.rsplit('.', 1)\n",
      "    if extension not in converter:\n",
      "        raise ImageComparisonFailure(\n",
      "            \"Don't know how to convert %s files to png\" % extension)\n",
      "    newname = base + '_' + extension + '.png'\n",
      "    if not os.path.exists(filename):\n",
      "        raise IOError(\"'%s' does not exist\" % filename)\n",
      "\n",
      "    # Only convert the file if the destination doesn't already exist or\n",
      "    # is out of date.\n",
      "    if (not os.path.exists(newname) or\n",
      "            os.stat(newname).st_mtime < os.stat(filename).st_mtime):\n",
      "        if cache:\n",
      "            cache_dir = get_cache_dir()\n",
      "        else:\n",
      "            cache_dir = None\n",
      "\n",
      "        if cache_dir is not None:\n",
      "            hash_value = get_file_hash(filename)\n",
      "            new_ext = os.path.splitext(newname)[1]\n",
      "            cached_file = os.path.join(cache_dir, hash_value + new_ext)\n",
      "            if os.path.exists(cached_file):\n",
      "                shutil.copyfile(cached_file, newname)\n",
      "                return newname\n",
      "\n",
      "        converter[extension](filename, newname)\n",
      "\n",
      "        if cache_dir is not None:\n",
      "            shutil.copyfile(newname, cached_file)\n",
      "\n",
      "    return newname\n",
      "\n",
      "#: Maps file extensions to a function which takes a filename as its\n",
      "#: only argument to return a list suitable for execution with Popen.\n",
      "#: The purpose of this is so that the result file (with the given\n",
      "#: extension) can be verified with tools such as xmllint for svg.\n",
      "verifiers = {}\n",
      "\n",
      "# Turning this off, because it seems to cause multiprocessing issues\n",
      "if matplotlib.checkdep_xmllint() and False:\n",
      "    verifiers['svg'] = lambda filename: [\n",
      "        'xmllint', '--valid', '--nowarning', '--noout', filename]\n",
      "\n",
      "\n",
      "def verify(filename):\n",
      "    \"\"\"Verify the file through some sort of verification tool.\"\"\"\n",
      "    if not os.path.exists(filename):\n",
      "        raise IOError(\"'%s' does not exist\" % filename)\n",
      "    base, extension = filename.rsplit('.', 1)\n",
      "    verifier = verifiers.get(extension, None)\n",
      "    if verifier is not None:\n",
      "        cmd = verifier(filename)\n",
      "        pipe = subprocess.Popen(\n",
      "            cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
      "        stdout, stderr = pipe.communicate()\n",
      "        errcode = pipe.wait()\n",
      "        if errcode != 0:\n",
      "            msg = \"File verification command failed:\\n%s\\n\" % ' '.join(cmd)\n",
      "            if stdout:\n",
      "                msg += \"Standard output:\\n%s\\n\" % stdout\n",
      "            if stderr:\n",
      "                msg += \"Standard error:\\n%s\\n\" % stderr\n",
      "            raise IOError(msg)\n",
      "\n",
      "\n",
      "def crop_to_same(actual_path, actual_image, expected_path, expected_image):\n",
      "    # clip the images to the same size -- this is useful only when\n",
      "    # comparing eps to pdf\n",
      "    if actual_path[-7:-4] == 'eps' and expected_path[-7:-4] == 'pdf':\n",
      "        aw, ah = actual_image.shape\n",
      "        ew, eh = expected_image.shape\n",
      "        actual_image = actual_image[int(aw / 2 - ew / 2):int(\n",
      "            aw / 2 + ew / 2), int(ah / 2 - eh / 2):int(ah / 2 + eh / 2)]\n",
      "    return actual_image, expected_image\n",
      "\n",
      "\n",
      "def calculate_rms(expectedImage, actualImage):\n",
      "    \"Calculate the per-pixel errors, then compute the root mean square error.\"\n",
      "    num_values = np.prod(expectedImage.shape)\n",
      "    abs_diff_image = abs(expectedImage - actualImage)\n",
      "\n",
      "    # On Numpy 1.6, we can use bincount with minlength, which is much\n",
      "    # faster than using histogram\n",
      "    expected_version = version.LooseVersion(\"1.6\")\n",
      "    found_version = version.LooseVersion(np.__version__)\n",
      "    if found_version >= expected_version:\n",
      "        histogram = np.bincount(abs_diff_image.ravel(), minlength=256)\n",
      "    else:\n",
      "        histogram = np.histogram(abs_diff_image, bins=np.arange(257))[0]\n",
      "\n",
      "    sum_of_squares = np.sum(histogram * np.arange(len(histogram)) ** 2)\n",
      "    rms = np.sqrt(float(sum_of_squares) / num_values)\n",
      "\n",
      "    return rms\n",
      "\n",
      "\n",
      "def compare_images(expected, actual, tol, in_decorator=False):\n",
      "    \"\"\"\n",
      "    Compare two \"image\" files checking differences within a tolerance.\n",
      "\n",
      "    The two given filenames may point to files which are convertible to\n",
      "    PNG via the `.converter` dictionary. The underlying RMS is calculated\n",
      "    with the `.calculate_rms` function.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    expected : str\n",
      "        The filename of the expected image.\n",
      "    actual :str\n",
      "        The filename of the actual image.\n",
      "    tol : float\n",
      "        The tolerance (a color value difference, where 255 is the\n",
      "        maximal difference).  The test fails if the average pixel\n",
      "        difference is greater than this value.\n",
      "    in_decorator : bool\n",
      "        If called from image_comparison decorator, this should be\n",
      "        True. (default=False)\n",
      "\n",
      "    Example\n",
      "    -------\n",
      "    img1 = \"./baseline/plot.png\"\n",
      "    img2 = \"./output/plot.png\"\n",
      "    compare_images( img1, img2, 0.001 ):\n",
      "\n",
      "    \"\"\"\n",
      "    if not os.path.exists(actual):\n",
      "        msg = \"Output image %s does not exist.\" % actual\n",
      "        raise Exception(msg)\n",
      "\n",
      "    if os.stat(actual).st_size == 0:\n",
      "        msg = \"Output image file %s is empty.\" % actual\n",
      "        raise Exception(msg)\n",
      "\n",
      "    verify(actual)\n",
      "\n",
      "    # Convert the image to png\n",
      "    extension = expected.split('.')[-1]\n",
      "\n",
      "    if not os.path.exists(expected):\n",
      "        raise IOError('Baseline image %r does not exist.' % expected)\n",
      "\n",
      "    if extension != 'png':\n",
      "        actual = convert(actual, False)\n",
      "        expected = convert(expected, True)\n",
      "\n",
      "    # open the image files and remove the alpha channel (if it exists)\n",
      "    expectedImage = _png.read_png_int(expected)\n",
      "    actualImage = _png.read_png_int(actual)\n",
      "    expectedImage = expectedImage[:, :, :3]\n",
      "    actualImage = actualImage[:, :, :3]\n",
      "\n",
      "    actualImage, expectedImage = crop_to_same(\n",
      "        actual, actualImage, expected, expectedImage)\n",
      "\n",
      "    # convert to signed integers, so that the images can be subtracted without\n",
      "    # overflow\n",
      "    expectedImage = expectedImage.astype(np.int16)\n",
      "    actualImage = actualImage.astype(np.int16)\n",
      "\n",
      "    rms = calculate_rms(expectedImage, actualImage)\n",
      "\n",
      "    diff_image = make_test_filename(actual, 'failed-diff')\n",
      "\n",
      "    if rms <= tol:\n",
      "        if os.path.exists(diff_image):\n",
      "            os.unlink(diff_image)\n",
      "        return None\n",
      "\n",
      "    save_diff_image(expected, actual, diff_image)\n",
      "\n",
      "    results = dict(rms=rms, expected=str(expected),\n",
      "                   actual=str(actual), diff=str(diff_image), tol=tol)\n",
      "\n",
      "    if not in_decorator:\n",
      "        # Then the results should be a string suitable for stdout.\n",
      "        template = ['Error: Image files did not match.',\n",
      "                    'RMS Value: {rms}',\n",
      "                    'Expected:  \\n    {expected}',\n",
      "                    'Actual:    \\n    {actual}',\n",
      "                    'Difference:\\n    {diff}',\n",
      "                    'Tolerance: \\n    {tol}', ]\n",
      "        results = '\\n  '.join([line.format(**results) for line in template])\n",
      "    return results\n",
      "\n",
      "\n",
      "def save_diff_image(expected, actual, output):\n",
      "    expectedImage = _png.read_png(expected)\n",
      "    actualImage = _png.read_png(actual)\n",
      "    actualImage, expectedImage = crop_to_same(\n",
      "        actual, actualImage, expected, expectedImage)\n",
      "    expectedImage = np.array(expectedImage).astype(np.float)\n",
      "    actualImage = np.array(actualImage).astype(np.float)\n",
      "    assert expectedImage.ndim == actualImage.ndim\n",
      "    assert expectedImage.shape == actualImage.shape\n",
      "    absDiffImage = abs(expectedImage - actualImage)\n",
      "\n",
      "    # expand differences in luminance domain\n",
      "    absDiffImage *= 255 * 10\n",
      "    save_image_np = np.clip(absDiffImage, 0, 255).astype(np.uint8)\n",
      "    height, width, depth = save_image_np.shape\n",
      "\n",
      "    # The PDF renderer doesn't produce an alpha channel, but the\n",
      "    # matplotlib PNG writer requires one, so expand the array\n",
      "    if depth == 3:\n",
      "        with_alpha = np.empty((height, width, 4), dtype=np.uint8)\n",
      "        with_alpha[:, :, 0:3] = save_image_np\n",
      "        save_image_np = with_alpha\n",
      "\n",
      "    # Hard-code the alpha channel to fully solid\n",
      "    save_image_np[:, :, 3] = 255\n",
      "\n",
      "    _png.write_png(save_image_np.tostring(), width, height, output)\n",
      "\n",
      "['*', '*', '*', '*', '*', '*', '*', '*', '*', '*']\n",
      "\"\"\"Test the TcEx Metrics Module.\"\"\"\n",
      "# standard library\n",
      "import uuid\n",
      "\n",
      "\n",
      "class TestMetrics:\n",
      "    \"\"\"Test the TcEx Metrics Module.\"\"\"\n",
      "\n",
      "    @staticmethod\n",
      "    def test_add_metrics(tcex):\n",
      "        \"\"\"Test **add** method of TcEx metrics module.\n",
      "\n",
      "        Args:\n",
      "            tcex (TcEx, fixture): An instantiated instance of TcEx object.\n",
      "        \"\"\"\n",
      "        date = '2008-12-12T12:12:12'\n",
      "        result_date = '2008-12-12T00:00:00Z'\n",
      "        metrics = tcex.metric(\n",
      "            name='pytest metrics',\n",
      "            description='pytest',\n",
      "            data_type='Average',\n",
      "            interval='Daily',\n",
      "            keyed=False,\n",
      "        )\n",
      "        results = metrics.add(value=42, date=date, return_value=True, weight=4)\n",
      "        metrics.add(value=24, date=date, return_value=False)\n",
      "        assert results.get('date') == result_date\n",
      "\n",
      "    @staticmethod\n",
      "    def test_add_keyed_metrics(tcex):\n",
      "        \"\"\"Test **add_keyed** method of TcEx metrics module.\n",
      "\n",
      "        Args:\n",
      "            tcex (TcEx, fixture): An instantiated instance of TcEx object.\n",
      "        \"\"\"\n",
      "        date = '2008-12-12T12:12:12'\n",
      "        result_date = '2008-12-12T00:00:00Z'\n",
      "        metrics = tcex.metric(\n",
      "            name='pytest metrics by Owner',\n",
      "            description='pytest',\n",
      "            data_type='Average',\n",
      "            interval='Daily',\n",
      "            keyed=True,\n",
      "        )\n",
      "        results = metrics.add_keyed(value=42, key='MyOrg', date=date, return_value=True, weight=4)\n",
      "        assert results.get('date') == result_date\n",
      "\n",
      "    @staticmethod\n",
      "    def test_create_new_metrics(tcex):\n",
      "        \"\"\"Test creation of a new metric\n",
      "\n",
      "        Args:\n",
      "            tcex (TcEx, fixture): An instantiated instance of TcEx object.\n",
      "        \"\"\"\n",
      "        name = f'pytest-{str(uuid.uuid4())}'\n",
      "        metrics = tcex.metric(\n",
      "            name=name, description='pytest', data_type='Sum', interval='Daily', keyed=True\n",
      "        )\n",
      "        assert metrics.metric_find()\n",
      "\n",
      "    @staticmethod\n",
      "    def test_create_new_metrics_fail(tcex):\n",
      "        \"\"\"Test failure of a new metric creation\n",
      "\n",
      "        Args:\n",
      "            tcex (TcEx, fixture): An instantiated instance of TcEx object.\n",
      "        \"\"\"\n",
      "        name = 'x' * 500\n",
      "        try:\n",
      "            tcex.metric(\n",
      "                name=name, description='pytest', data_type='Sum', interval='Daily', keyed=True\n",
      "            )\n",
      "            assert False, 'Failed to catch API error for metric name to long'\n",
      "        except RuntimeError:\n",
      "            assert True\n",
      "\n",
      "['*', '*', '*', '*', '*', '*', '*', '*', '*', '*']\n",
      "\"\"\"Test the TcEx Threat Intel Module.\"\"\"\n",
      "# standard library\n",
      "import os\n",
      "from random import randint\n",
      "\n",
      "from .ti_helpers import TestThreatIntelligence, TIHelper\n",
      "\n",
      "\n",
      "class TestAsnIndicators(TestThreatIntelligence):\n",
      "    \"\"\"Test TcEx ASN Indicators.\"\"\"\n",
      "\n",
      "    indicator_field = 'AS Number'\n",
      "    indicator_field_arg = indicator_field.replace(' ', '_').lower()\n",
      "    indicator_type = 'ASN'\n",
      "    owner = os.getenv('TC_OWNER')\n",
      "    ti = None\n",
      "    ti_helper = None\n",
      "    tcex = None\n",
      "\n",
      "    def setup_method(self):\n",
      "        \"\"\"Configure setup before all tests.\"\"\"\n",
      "        self.ti_helper = TIHelper(self.indicator_type, self.indicator_field_arg)\n",
      "        self.ti = self.ti_helper.ti\n",
      "        self.tcex = self.ti_helper.tcex\n",
      "\n",
      "    def teardown_method(self):\n",
      "        \"\"\"Configure teardown before all tests.\"\"\"\n",
      "        if os.getenv('TEARDOWN_METHOD') is None:\n",
      "            self.ti_helper.cleanup()\n",
      "\n",
      "    def tests_ti_asn_create(self):\n",
      "        \"\"\"Create an indicator using specific interface.\"\"\"\n",
      "        indicator_data = {\n",
      "            self.indicator_field_arg: self.ti_helper.rand_asn(),\n",
      "            'confidence': randint(0, 100),\n",
      "            'owner': self.owner,\n",
      "            'rating': randint(0, 5),\n",
      "        }\n",
      "        ti = self.ti.asn(**indicator_data)\n",
      "        r = ti.create()\n",
      "\n",
      "        # assert response\n",
      "        assert r.status_code == 201\n",
      "\n",
      "        # retrieve indicator for asserts\n",
      "        ti = self.ti.asn(**indicator_data)\n",
      "        r = ti.single()\n",
      "        response_data = r.json()\n",
      "        ti_data = response_data.get('data', {}).get(ti.api_entity)\n",
      "\n",
      "        # validate response data\n",
      "        assert r.status_code == 200\n",
      "        assert response_data.get('status') == 'Success'\n",
      "\n",
      "        # validate ti data\n",
      "        assert ti_data.get('confidence') == indicator_data.get('confidence')\n",
      "        assert ti_data.get(self.indicator_field) == indicator_data.get(self.indicator_field_arg)\n",
      "        assert ti_data.get('rating') == indicator_data.get('rating')\n",
      "\n",
      "        # cleanup indicator\n",
      "        r = ti.delete()\n",
      "        assert r.status_code == 200\n",
      "\n",
      "    def tests_ti_asn_add_attribute(self, request):\n",
      "        \"\"\"Test indicator add attribute.\"\"\"\n",
      "        super().indicator_add_attribute(request)\n",
      "\n",
      "    def tests_ti_asn_add_label(self):\n",
      "        \"\"\"Test indicator add label.\"\"\"\n",
      "        super().indicator_add_label()\n",
      "\n",
      "    def tests_ti_asn_add_tag(self, request):\n",
      "        \"\"\"Test indicator add tag.\"\"\"\n",
      "        super().indicator_add_tag(request)\n",
      "\n",
      "    def tests_ti_asn_delete(self):\n",
      "        \"\"\"Test indicator delete.\"\"\"\n",
      "        super().indicator_delete()\n",
      "\n",
      "    def tests_ti_asn_get(self):\n",
      "        \"\"\"Test indicator get with generic indicator method.\"\"\"\n",
      "        super().indicator_get()\n",
      "\n",
      "    def tests_ti_asn_get_includes(self, request):\n",
      "        \"\"\"Test indicator get with includes.\"\"\"\n",
      "        super().indicator_get_includes(request)\n",
      "\n",
      "    def tests_ti_asn_get_attribute(self, request):\n",
      "        \"\"\"Test indicator get attribute.\"\"\"\n",
      "        super().indicator_get_attribute(request)\n",
      "\n",
      "    def tests_ti_asn_get_label(self):\n",
      "        \"\"\"Test indicator get label.\"\"\"\n",
      "        super().indicator_get_label()\n",
      "\n",
      "    def tests_ti_asn_get_tag(self, request):\n",
      "        \"\"\"Test indicator get tag.\"\"\"\n",
      "        super().indicator_get_tag(request)\n",
      "\n",
      "    def tests_ti_asn_update(self):\n",
      "        \"\"\"Test updating indicator metadata.\"\"\"\n",
      "        super().indicator_update()\n",
      "\n",
      "['*', '*', '*', '*', '*', '*', '*', '*', '*', '*']\n",
      "from __future__ import unicode_literals\n",
      "\n",
      "import json\n",
      "import re\n",
      "import time\n",
      "import warnings\n",
      "import hashlib\n",
      "\n",
      "from django import forms\n",
      "from django.conf import settings\n",
      "from django.contrib import messages\n",
      "from django.contrib.auth import login as django_login, get_backends\n",
      "from django.contrib.auth import logout as django_logout, authenticate\n",
      "from django.contrib.auth.models import AbstractUser\n",
      "from django.core.cache import cache\n",
      "from django.core.mail import EmailMultiAlternatives, EmailMessage\n",
      "from django.core.urlresolvers import reverse\n",
      "from django.http import HttpResponse\n",
      "from django.http import HttpResponseRedirect\n",
      "from django.template.loader import render_to_string\n",
      "from django.template import TemplateDoesNotExist\n",
      "from django.utils import timezone\n",
      "from django.utils.translation import ugettext_lazy as _\n",
      "\n",
      "try:\n",
      "    from django.utils.encoding import force_text\n",
      "except ImportError:\n",
      "    from django.utils.encoding import force_unicode as force_text\n",
      "\n",
      "from ..compat import validate_password\n",
      "from ..utils import (build_absolute_uri, get_current_site,\n",
      "                     generate_unique_username,\n",
      "                     get_user_model, import_attribute,\n",
      "                     resolve_url, email_address_exists)\n",
      "\n",
      "from . import app_settings\n",
      "\n",
      "\n",
      "class DefaultAccountAdapter(object):\n",
      "\n",
      "    # Don't bother turning this into a setting, as changing this also\n",
      "    # requires changing the accompanying form error message. So if you\n",
      "    # need to change any of this, simply override clean_username().\n",
      "    username_regex = re.compile(r'^[\\w.@+-]+$')\n",
      "    error_messages = {\n",
      "        'invalid_username':\n",
      "        _('Usernames can only contain letters, digits and @/./+/-/_.'),\n",
      "        'username_blacklisted':\n",
      "        _('Username can not be used. Please use other username.'),\n",
      "        'username_taken':\n",
      "        AbstractUser._meta.get_field('username').error_messages['unique'],\n",
      "        'too_many_login_attempts':\n",
      "        _('Too many failed login attempts. Try again later.'),\n",
      "        'email_taken':\n",
      "        _(\"A user is already registered with this e-mail address.\"),\n",
      "    }\n",
      "\n",
      "    def __init__(self, request=None):\n",
      "        self.request = request\n",
      "\n",
      "    def stash_verified_email(self, request, email):\n",
      "        request.session['account_verified_email'] = email\n",
      "\n",
      "    def unstash_verified_email(self, request):\n",
      "        ret = request.session.get('account_verified_email')\n",
      "        request.session['account_verified_email'] = None\n",
      "        return ret\n",
      "\n",
      "    def stash_user(self, request, user):\n",
      "        request.session['account_user'] = user\n",
      "\n",
      "    def unstash_user(self, request):\n",
      "        return request.session.pop('account_user', None)\n",
      "\n",
      "    def is_email_verified(self, request, email):\n",
      "        \"\"\"\n",
      "        Checks whether or not the email address is already verified\n",
      "        beyond allauth scope, for example, by having accepted an\n",
      "        invitation before signing up.\n",
      "        \"\"\"\n",
      "        ret = False\n",
      "        verified_email = request.session.get('account_verified_email')\n",
      "        if verified_email:\n",
      "            ret = verified_email.lower() == email.lower()\n",
      "        return ret\n",
      "\n",
      "    def format_email_subject(self, subject):\n",
      "        prefix = app_settings.EMAIL_SUBJECT_PREFIX\n",
      "        if prefix is None:\n",
      "            site = get_current_site(self.request)\n",
      "            prefix = \"[{name}] \".format(name=site.name)\n",
      "        return prefix + force_text(subject)\n",
      "\n",
      "    def get_from_email(self):\n",
      "        \"\"\"\n",
      "        This is a hook that can be overridden to programatically\n",
      "        set the 'from' email address for sending emails\n",
      "        \"\"\"\n",
      "        return settings.DEFAULT_FROM_EMAIL\n",
      "\n",
      "    def render_mail(self, template_prefix, email, context):\n",
      "        \"\"\"\n",
      "        Renders an e-mail to `email`.  `template_prefix` identifies the\n",
      "        e-mail that is to be sent, e.g. \"account/email/email_confirmation\"\n",
      "        \"\"\"\n",
      "        subject = render_to_string('{0}_subject.txt'.format(template_prefix),\n",
      "                                   context)\n",
      "        # remove superfluous line breaks\n",
      "        subject = \" \".join(subject.splitlines()).strip()\n",
      "        subject = self.format_email_subject(subject)\n",
      "\n",
      "        from_email = self.get_from_email()\n",
      "\n",
      "        bodies = {}\n",
      "        for ext in ['html', 'txt']:\n",
      "            try:\n",
      "                template_name = '{0}_message.{1}'.format(template_prefix, ext)\n",
      "                bodies[ext] = render_to_string(template_name,\n",
      "                                               context).strip()\n",
      "            except TemplateDoesNotExist:\n",
      "                if ext == 'txt' and not bodies:\n",
      "                    # We need at least one body\n",
      "                    raise\n",
      "        if 'txt' in bodies:\n",
      "            msg = EmailMultiAlternatives(subject,\n",
      "                                         bodies['txt'],\n",
      "                                         from_email,\n",
      "                                         [email])\n",
      "            if 'html' in bodies:\n",
      "                msg.attach_alternative(bodies['html'], 'text/html')\n",
      "        else:\n",
      "            msg = EmailMessage(subject,\n",
      "                               bodies['html'],\n",
      "                               from_email,\n",
      "                               [email])\n",
      "            msg.content_subtype = 'html'  # Main content is now text/html\n",
      "        return msg\n",
      "\n",
      "    def send_mail(self, template_prefix, email, context):\n",
      "        msg = self.render_mail(template_prefix, email, context)\n",
      "        msg.send()\n",
      "\n",
      "    def get_login_redirect_url(self, request):\n",
      "        \"\"\"\n",
      "        Returns the default URL to redirect to after logging in.  Note\n",
      "        that URLs passed explicitly (e.g. by passing along a `next`\n",
      "        GET parameter) take precedence over the value returned here.\n",
      "        \"\"\"\n",
      "        assert request.user.is_authenticated()\n",
      "        url = getattr(settings, \"LOGIN_REDIRECT_URLNAME\", None)\n",
      "        if url:\n",
      "            warnings.warn(\"LOGIN_REDIRECT_URLNAME is deprecated, simply\"\n",
      "                          \" use LOGIN_REDIRECT_URL with a URL name\",\n",
      "                          DeprecationWarning)\n",
      "        else:\n",
      "            url = settings.LOGIN_REDIRECT_URL\n",
      "        return resolve_url(url)\n",
      "\n",
      "    def get_logout_redirect_url(self, request):\n",
      "        \"\"\"\n",
      "        Returns the URL to redirect to after the user logs out. Note that\n",
      "        this method is also invoked if you attempt to log out while no users\n",
      "        is logged in. Therefore, request.user is not guaranteed to be an\n",
      "        authenticated user.\n",
      "        \"\"\"\n",
      "        return resolve_url(app_settings.LOGOUT_REDIRECT_URL)\n",
      "\n",
      "    def get_email_confirmation_redirect_url(self, request):\n",
      "        \"\"\"\n",
      "        The URL to return to after successful e-mail confirmation.\n",
      "        \"\"\"\n",
      "        if request.user.is_authenticated():\n",
      "            if app_settings.EMAIL_CONFIRMATION_AUTHENTICATED_REDIRECT_URL:\n",
      "                return  \\\n",
      "                    app_settings.EMAIL_CONFIRMATION_AUTHENTICATED_REDIRECT_URL\n",
      "            else:\n",
      "                return self.get_login_redirect_url(request)\n",
      "        else:\n",
      "            return app_settings.EMAIL_CONFIRMATION_ANONYMOUS_REDIRECT_URL\n",
      "\n",
      "    def is_open_for_signup(self, request):\n",
      "        \"\"\"\n",
      "        Checks whether or not the site is open for signups.\n",
      "\n",
      "        Next to simply returning True/False you can also intervene the\n",
      "        regular flow by raising an ImmediateHttpResponse\n",
      "        \"\"\"\n",
      "        return True\n",
      "\n",
      "    def new_user(self, request):\n",
      "        \"\"\"\n",
      "        Instantiates a new User instance.\n",
      "        \"\"\"\n",
      "        user = get_user_model()()\n",
      "        return user\n",
      "\n",
      "    def populate_username(self, request, user):\n",
      "        \"\"\"\n",
      "        Fills in a valid username, if required and missing.  If the\n",
      "        username is already present it is assumed to be valid\n",
      "        (unique).\n",
      "        \"\"\"\n",
      "        from .utils import user_username, user_email, user_field\n",
      "        first_name = user_field(user, 'first_name')\n",
      "        last_name = user_field(user, 'last_name')\n",
      "        email = user_email(user)\n",
      "        username = user_username(user)\n",
      "        if app_settings.USER_MODEL_USERNAME_FIELD:\n",
      "            user_username(\n",
      "                user,\n",
      "                username or self.generate_unique_username([\n",
      "                    first_name,\n",
      "                    last_name,\n",
      "                    email,\n",
      "                    username,\n",
      "                    'user']))\n",
      "\n",
      "    def generate_unique_username(self, txts, regex=None):\n",
      "        return generate_unique_username(txts, regex)\n",
      "\n",
      "    def save_user(self, request, user, form, commit=True):\n",
      "        \"\"\"\n",
      "        Saves a new `User` instance using information provided in the\n",
      "        signup form.\n",
      "        \"\"\"\n",
      "        from .utils import user_username, user_email, user_field\n",
      "\n",
      "        data = form.cleaned_data\n",
      "        first_name = data.get('first_name')\n",
      "        last_name = data.get('last_name')\n",
      "        email = data.get('email')\n",
      "        username = data.get('username')\n",
      "        user_email(user, email)\n",
      "        user_username(user, username)\n",
      "        if first_name:\n",
      "            user_field(user, 'first_name', first_name)\n",
      "        if last_name:\n",
      "            user_field(user, 'last_name', last_name)\n",
      "        if 'password1' in data:\n",
      "            user.set_password(data[\"password1\"])\n",
      "        else:\n",
      "            user.set_unusable_password()\n",
      "        self.populate_username(request, user)\n",
      "        if commit:\n",
      "            # Ability not to commit makes it easier to derive from\n",
      "            # this adapter by adding\n",
      "            user.save()\n",
      "        return user\n",
      "\n",
      "    def clean_username(self, username, shallow=False):\n",
      "        \"\"\"\n",
      "        Validates the username. You can hook into this if you want to\n",
      "        (dynamically) restrict what usernames can be chosen.\n",
      "        \"\"\"\n",
      "        if not self.username_regex.match(username):\n",
      "            raise forms.ValidationError(\n",
      "                self.error_messages['invalid_username'])\n",
      "\n",
      "        # TODO: Add regexp support to USERNAME_BLACKLIST\n",
      "        username_blacklist_lower = [ub.lower()\n",
      "                                    for ub in app_settings.USERNAME_BLACKLIST]\n",
      "        if username.lower() in username_blacklist_lower:\n",
      "            raise forms.ValidationError(\n",
      "                self.error_messages['username_blacklisted'])\n",
      "        # Skipping database lookups when shallow is True, needed for unique\n",
      "        # username generation.\n",
      "        if not shallow:\n",
      "            username_field = app_settings.USER_MODEL_USERNAME_FIELD\n",
      "            assert username_field\n",
      "            user_model = get_user_model()\n",
      "            try:\n",
      "                query = {username_field + '__iexact': username}\n",
      "                user_model.objects.get(**query)\n",
      "            except user_model.DoesNotExist:\n",
      "                return username\n",
      "            error_message = user_model._meta.get_field(\n",
      "                username_field).error_messages.get('unique')\n",
      "            if not error_message:\n",
      "                error_message = self.error_messages['username_taken']\n",
      "            raise forms.ValidationError(error_message)\n",
      "        return username\n",
      "\n",
      "    def clean_email(self, email):\n",
      "        \"\"\"\n",
      "        Validates an email value. You can hook into this if you want to\n",
      "        (dynamically) restrict what email addresses can be chosen.\n",
      "        \"\"\"\n",
      "        return email\n",
      "\n",
      "    def clean_password(self, password, user=None):\n",
      "        \"\"\"\n",
      "        Validates a password. You can hook into this if you want to\n",
      "        restric the allowed password choices.\n",
      "        \"\"\"\n",
      "        min_length = app_settings.PASSWORD_MIN_LENGTH\n",
      "        if min_length and len(password) < min_length:\n",
      "            raise forms.ValidationError(_(\"Password must be a minimum of {0} \"\n",
      "                                          \"characters.\").format(min_length))\n",
      "        validate_password(password, user)\n",
      "        return password\n",
      "\n",
      "    def validate_unique_email(self, email):\n",
      "        if email_address_exists(email):\n",
      "            raise forms.ValidationError(self.error_messages['email_taken'])\n",
      "        return email\n",
      "\n",
      "    def add_message(self, request, level, message_template,\n",
      "                    message_context=None, extra_tags=''):\n",
      "        \"\"\"\n",
      "        Wrapper of `django.contrib.messages.add_message`, that reads\n",
      "        the message text from a template.\n",
      "        \"\"\"\n",
      "        if 'django.contrib.messages' in settings.INSTALLED_APPS:\n",
      "            try:\n",
      "                if message_context is None:\n",
      "                    message_context = {}\n",
      "                message = render_to_string(message_template,\n",
      "                                           message_context).strip()\n",
      "                if message:\n",
      "                    messages.add_message(request, level, message,\n",
      "                                         extra_tags=extra_tags)\n",
      "            except TemplateDoesNotExist:\n",
      "                pass\n",
      "\n",
      "    def ajax_response(self, request, response, redirect_to=None, form=None):\n",
      "        data = {}\n",
      "        status = response.status_code\n",
      "\n",
      "        if redirect_to:\n",
      "            status = 200\n",
      "            data['location'] = redirect_to\n",
      "        if form:\n",
      "            if form.is_valid():\n",
      "                status = 200\n",
      "            else:\n",
      "                status = 400\n",
      "                data['form_errors'] = form._errors\n",
      "            if hasattr(response, 'render'):\n",
      "                response.render()\n",
      "            data['html'] = response.content.decode('utf8')\n",
      "        return HttpResponse(json.dumps(data),\n",
      "                            status=status,\n",
      "                            content_type='application/json')\n",
      "\n",
      "    def login(self, request, user):\n",
      "        # HACK: This is not nice. The proper Django way is to use an\n",
      "        # authentication backend\n",
      "        if not hasattr(user, 'backend'):\n",
      "            from .auth_backends import AuthenticationBackend\n",
      "            backends = get_backends()\n",
      "            for backend in backends:\n",
      "                if isinstance(backend, AuthenticationBackend):\n",
      "                    # prefer our own backend\n",
      "                    break\n",
      "            else:\n",
      "                # Pick one\n",
      "                backend = backends[0]\n",
      "            backend_path = '.'.join([backend.__module__,\n",
      "                                     backend.__class__.__name__])\n",
      "            user.backend = backend_path\n",
      "        django_login(request, user)\n",
      "\n",
      "    def logout(self, request):\n",
      "        django_logout(request)\n",
      "\n",
      "    def confirm_email(self, request, email_address):\n",
      "        \"\"\"\n",
      "        Marks the email address as confirmed on the db\n",
      "        \"\"\"\n",
      "        email_address.verified = True\n",
      "        email_address.set_as_primary(conditional=True)\n",
      "        email_address.save()\n",
      "\n",
      "    def set_password(self, user, password):\n",
      "        user.set_password(password)\n",
      "        user.save()\n",
      "\n",
      "    def get_user_search_fields(self):\n",
      "        user = get_user_model()()\n",
      "        return filter(lambda a: a and hasattr(user, a),\n",
      "                      [app_settings.USER_MODEL_USERNAME_FIELD,\n",
      "                       'first_name', 'last_name', 'email'])\n",
      "\n",
      "    def is_safe_url(self, url):\n",
      "        from django.utils.http import is_safe_url\n",
      "        return is_safe_url(url)\n",
      "\n",
      "    def get_email_confirmation_url(self, request, emailconfirmation):\n",
      "        \"\"\"Constructs the email confirmation (activation) url.\n",
      "\n",
      "        Note that if you have architected your system such that email\n",
      "        confirmations are sent outside of the request context `request`\n",
      "        can be `None` here.\n",
      "        \"\"\"\n",
      "        url = reverse(\n",
      "            \"account_confirm_email\",\n",
      "            args=[emailconfirmation.key])\n",
      "        ret = build_absolute_uri(\n",
      "            request,\n",
      "            url)\n",
      "        return ret\n",
      "\n",
      "    def send_confirmation_mail(self, request, emailconfirmation, signup):\n",
      "        current_site = get_current_site(request)\n",
      "        activate_url = self.get_email_confirmation_url(\n",
      "            request,\n",
      "            emailconfirmation)\n",
      "        ctx = {\n",
      "            \"user\": emailconfirmation.email_address.user,\n",
      "            \"activate_url\": activate_url,\n",
      "            \"current_site\": current_site,\n",
      "            \"key\": emailconfirmation.key,\n",
      "        }\n",
      "        if signup:\n",
      "            email_template = 'account/email/email_confirmation_signup'\n",
      "        else:\n",
      "            email_template = 'account/email/email_confirmation'\n",
      "        self.send_mail(email_template,\n",
      "                       emailconfirmation.email_address.email,\n",
      "                       ctx)\n",
      "\n",
      "    def respond_user_inactive(self, request, user):\n",
      "        return HttpResponseRedirect(\n",
      "            reverse('account_inactive'))\n",
      "\n",
      "    def respond_email_verification_sent(self, request, user):\n",
      "        return HttpResponseRedirect(\n",
      "            reverse('account_email_verification_sent'))\n",
      "\n",
      "    def _get_login_attempts_cache_key(self, request, **credentials):\n",
      "        site = get_current_site(request)\n",
      "        login = credentials.get('email', credentials.get('username', ''))\n",
      "        login_key = hashlib.sha256(login.encode('utf8')).hexdigest()\n",
      "        return 'allauth/login_attempts@{site_id}:{login}'.format(\n",
      "            site_id=site.pk,\n",
      "            login=login_key)\n",
      "\n",
      "    def pre_authenticate(self, request, **credentials):\n",
      "        if app_settings.LOGIN_ATTEMPTS_LIMIT:\n",
      "            cache_key = self._get_login_attempts_cache_key(\n",
      "                request, **credentials)\n",
      "            login_data = cache.get(cache_key, None)\n",
      "            if login_data:\n",
      "                dt = timezone.now()\n",
      "                current_attempt_time = time.mktime(dt.timetuple())\n",
      "                if (len(login_data) >= app_settings.LOGIN_ATTEMPTS_LIMIT and\n",
      "                        current_attempt_time < (\n",
      "                            login_data[-1] +\n",
      "                            app_settings.LOGIN_ATTEMPTS_TIMEOUT)):\n",
      "                    raise forms.ValidationError(\n",
      "                        self.error_messages['too_many_login_attempts'])\n",
      "\n",
      "    def authenticate(self, request, **credentials):\n",
      "        \"\"\"Only authenticates, does not actually login. See `login`\"\"\"\n",
      "        self.pre_authenticate(request, **credentials)\n",
      "        user = authenticate(**credentials)\n",
      "        if user:\n",
      "            cache_key = self._get_login_attempts_cache_key(\n",
      "                request, **credentials)\n",
      "            cache.delete(cache_key)\n",
      "        else:\n",
      "            self.authentication_failed(request, **credentials)\n",
      "        return user\n",
      "\n",
      "    def authentication_failed(self, request, **credentials):\n",
      "        cache_key = self._get_login_attempts_cache_key(request, **credentials)\n",
      "        data = cache.get(cache_key, [])\n",
      "        dt = timezone.now()\n",
      "        data.append(time.mktime(dt.timetuple()))\n",
      "        cache.set(cache_key, data, app_settings.LOGIN_ATTEMPTS_TIMEOUT)\n",
      "\n",
      "\n",
      "def get_adapter(request=None):\n",
      "    return import_attribute(app_settings.ADAPTER)(request)\n",
      "\n",
      "['*', '*', '*', '*', '*', '*', '*', '*', '*', '*']\n",
      "# -*- coding: utf-8 -*-\n",
      "#\n",
      "\"\"\"\n",
      "This is an RDFLib store around Ivan Herman et al.'s SPARQL service wrapper.\n",
      "This was first done in layer-cake, and then ported to RDFLib\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "# Defines some SPARQL keywords\n",
      "LIMIT = 'LIMIT'\n",
      "OFFSET = 'OFFSET'\n",
      "ORDERBY = 'ORDER BY'\n",
      "\n",
      "import re\n",
      "import collections\n",
      "import urllib2\n",
      "\n",
      "# import warnings\n",
      "try:\n",
      "    from SPARQLWrapper import SPARQLWrapper, XML, POST, GET, URLENCODED, POSTDIRECTLY\n",
      "except ImportError:\n",
      "    raise Exception(\n",
      "        \"SPARQLWrapper not found! SPARQL Store will not work.\" +\n",
      "        \"Install with 'easy_install SPARQLWrapper'\")\n",
      "\n",
      "import sys\n",
      "if getattr(sys, 'pypy_version_info', None) is not None \\\n",
      "    or sys.platform.startswith('java') \\\n",
      "        or sys.version_info[:2] < (2, 6):\n",
      "    # import elementtree as etree\n",
      "    from elementtree import ElementTree\n",
      "    assert ElementTree\n",
      "else:\n",
      "    try:\n",
      "        from xml.etree import ElementTree\n",
      "        assert ElementTree\n",
      "    except ImportError:\n",
      "        from elementtree import ElementTree\n",
      "\n",
      "from rdflib.plugins.stores.regexmatching import NATIVE_REGEX\n",
      "\n",
      "from rdflib.store import Store\n",
      "from rdflib.query import Result\n",
      "from rdflib import Variable, Namespace, BNode, URIRef, Literal\n",
      "from rdflib.graph import DATASET_DEFAULT_GRAPH_ID\n",
      "\n",
      "import httplib\n",
      "import urlparse\n",
      "\n",
      "class NSSPARQLWrapper(SPARQLWrapper):\n",
      "    nsBindings = {}\n",
      "\n",
      "    def setNamespaceBindings(self, bindings):\n",
      "        \"\"\"\n",
      "        A shortcut for setting namespace bindings that will be added\n",
      "        to the prolog of the query\n",
      "\n",
      "        @param bindings: A dictionary of prefixs to URIs\n",
      "        \"\"\"\n",
      "        self.nsBindings.update(bindings)\n",
      "\n",
      "    def setQuery(self, query):\n",
      "        \"\"\"\n",
      "        Set the SPARQL query text. Note: no check is done on the\n",
      "        validity of the query (syntax or otherwise) by this module,\n",
      "        except for testing the query type (SELECT, ASK, etc).\n",
      "\n",
      "        Syntax and validity checking is done by the SPARQL service itself.\n",
      "\n",
      "        @param query: query text\n",
      "        @type query: string\n",
      "        @bug: #2320024\n",
      "        \"\"\"\n",
      "        self.queryType = self._parseQueryType(query)\n",
      "        self.queryString = self.injectPrefixes(query)\n",
      "\n",
      "    def injectPrefixes(self, query):\n",
      "        return '\\n'.join(\n",
      "            ['\\n'.join(['PREFIX %s: <%s>' % (key, val)\n",
      "                        for key, val in self.nsBindings.items()]),\n",
      "             query])\n",
      "\n",
      "BNODE_IDENT_PATTERN = re.compile('(?P<label>_\\:[^\\s]+)')\n",
      "SPARQL_NS = Namespace('http://www.w3.org/2005/sparql-results#')\n",
      "sparqlNsBindings = {u'sparql': SPARQL_NS}\n",
      "ElementTree._namespace_map[\"sparql\"] = SPARQL_NS\n",
      "\n",
      "\n",
      "def TraverseSPARQLResultDOM(doc, asDictionary=False):\n",
      "    \"\"\"\n",
      "    Returns a generator over tuples of results\n",
      "    \"\"\"\n",
      "    # namespace handling in elementtree xpath sub-set is not pretty :(\n",
      "    vars = [Variable(v.attrib[\"name\"]) for v in doc.findall(\n",
      "            './{http://www.w3.org/2005/sparql-results#}head/' +\n",
      "            '{http://www.w3.org/2005/sparql-results#}variable')]\n",
      "    for result in doc.findall(\n",
      "            './{http://www.w3.org/2005/sparql-results#}results/' +\n",
      "            '{http://www.w3.org/2005/sparql-results#}result'):\n",
      "        currBind = {}\n",
      "        values = []\n",
      "        for binding in result.findall(\n",
      "                '{http://www.w3.org/2005/sparql-results#}binding'):\n",
      "            varVal = binding.attrib[\"name\"]\n",
      "            var = Variable(varVal)\n",
      "            term = CastToTerm(binding.findall('*')[0])\n",
      "            values.append(term)\n",
      "            currBind[var] = term\n",
      "        if asDictionary:\n",
      "            yield currBind, vars\n",
      "        else:\n",
      "            def __locproc(values):\n",
      "                if len(values) == 1:\n",
      "                    return values[0]\n",
      "                else:\n",
      "                    return tuple(values)\n",
      "            yield __locproc(values), vars\n",
      "\n",
      "\n",
      "def localName(qname):\n",
      "    # wtf - elementtree cant do this for me\n",
      "    return qname[qname.index(\"}\") + 1:]\n",
      "\n",
      "\n",
      "def CastToTerm(node):\n",
      "    \"\"\"\n",
      "    Helper function that casts XML node in SPARQL results\n",
      "    to appropriate rdflib term\n",
      "    \"\"\"\n",
      "    if node.tag == '{%s}bnode' % SPARQL_NS:\n",
      "        return BNode(node.text)\n",
      "    elif node.tag == '{%s}uri' % SPARQL_NS:\n",
      "        return URIRef(node.text)\n",
      "    elif node.tag == '{%s}literal' % SPARQL_NS:\n",
      "        value = node.text if node.text is not None else ''\n",
      "        if 'datatype' in node.attrib:\n",
      "            dT = URIRef(node.attrib['datatype'])\n",
      "            return Literal(value, datatype=dT)\n",
      "        elif '{http://www.w3.org/XML/1998/namespace}lang' in node.attrib:\n",
      "            return Literal(value, lang=node.attrib[\n",
      "                \"{http://www.w3.org/XML/1998/namespace}lang\"])\n",
      "        else:\n",
      "            return Literal(value)\n",
      "    else:\n",
      "        raise Exception('Unknown answer type')\n",
      "\n",
      "\n",
      "class SPARQLStore(NSSPARQLWrapper, Store):\n",
      "    \"\"\"\n",
      "    An RDFLib store around a SPARQL endpoint\n",
      "\n",
      "    This is in theory context-aware and should work as expected\n",
      "    when a context is specified.\n",
      "\n",
      "    For ConjunctiveGraphs, reading is done from the \"default graph\". Exactly\n",
      "    what this means depends on your endpoint, because SPARQL does not offer a\n",
      "    simple way to query the union of all graphs as it would be expected for a\n",
      "    ConjuntiveGraph. This is why we recommend using Dataset instead, which is\n",
      "    motivated by the SPARQL 1.1.\n",
      "\n",
      "    Fuseki/TDB has a flag for specifying that the default graph\n",
      "    is the union of all graphs (tdb:unionDefaultGraph in the Fuseki config).\n",
      "\n",
      "    .. warning:: The SPARQL Store does not support blank-nodes!\n",
      "\n",
      "                 As blank-nodes act as variables in SPARQL queries\n",
      "                 there is no way to query for a particular blank node.\n",
      "\n",
      "                 See http://www.w3.org/TR/sparql11-query/#BGPsparqlBNodes\n",
      "\n",
      "\n",
      "    \"\"\"\n",
      "    formula_aware = False\n",
      "    transaction_aware = False\n",
      "    graph_aware = True\n",
      "    regex_matching = NATIVE_REGEX\n",
      "\n",
      "    def __init__(self,\n",
      "                 endpoint=None, bNodeAsURI=False,\n",
      "                 sparql11=True, context_aware=True,\n",
      "                 **sparqlwrapper_kwargs):\n",
      "        \"\"\"\n",
      "        \"\"\"\n",
      "        super(SPARQLStore, self).__init__(endpoint, returnFormat=XML, **sparqlwrapper_kwargs)\n",
      "        self.setUseKeepAlive()\n",
      "        self.bNodeAsURI = bNodeAsURI\n",
      "        self.nsBindings = {}\n",
      "        self.sparql11 = sparql11\n",
      "        self.context_aware = context_aware\n",
      "        self.graph_aware = context_aware\n",
      "\n",
      "    # Database Management Methods\n",
      "    def create(self, configuration):\n",
      "        raise TypeError('The SPARQL store is read only')\n",
      "\n",
      "    def open(self, configuration, create=False):\n",
      "        \"\"\"\n",
      "        sets the endpoint URL for this SPARQLStore\n",
      "        if create==True an exception is thrown.\n",
      "        \"\"\"\n",
      "        if create:\n",
      "            raise Exception(\"Cannot create a SPARQL Endpoint\")\n",
      "\n",
      "        self.query_endpoint = configuration\n",
      "\n",
      "    def __set_query_endpoint(self, queryEndpoint):\n",
      "        super(SPARQLStore, self).__init__(queryEndpoint, returnFormat=XML)\n",
      "        self.endpoint = queryEndpoint\n",
      "\n",
      "    def __get_query_endpoint(self):\n",
      "        return self.endpoint\n",
      "\n",
      "    query_endpoint = property(__get_query_endpoint, __set_query_endpoint)\n",
      "\n",
      "    def destroy(self, configuration):\n",
      "        raise TypeError('The SPARQL store is read only')\n",
      "\n",
      "    # Transactional interfaces\n",
      "    def commit(self):\n",
      "        raise TypeError('The SPARQL store is read only')\n",
      "\n",
      "    def rollback(self):\n",
      "        raise TypeError('The SPARQL store is read only')\n",
      "\n",
      "    def add(self, (subject, predicate, obj), context=None, quoted=False):\n",
      "        raise TypeError('The SPARQL store is read only')\n",
      "\n",
      "    def addN(self, quads):\n",
      "        raise TypeError('The SPARQL store is read only')\n",
      "\n",
      "    def remove(self, (subject, predicate, obj), context):\n",
      "        raise TypeError('The SPARQL store is read only')\n",
      "\n",
      "    def query(self, query,\n",
      "              initNs={},\n",
      "              initBindings={},\n",
      "              queryGraph=None,\n",
      "              DEBUG=False):\n",
      "        self.debug = DEBUG\n",
      "        assert isinstance(query, basestring)\n",
      "        self.setNamespaceBindings(initNs)\n",
      "        if initBindings:\n",
      "            if not self.sparql11:\n",
      "                raise Exception(\n",
      "                    \"initBindings not supported for SPARQL 1.0 Endpoints.\")\n",
      "            v = list(initBindings)\n",
      "\n",
      "            # VALUES was added to SPARQL 1.1 on 2012/07/24\n",
      "            query += \"\\nVALUES ( %s )\\n{ ( %s ) }\\n\"\\\n",
      "                % (\" \".join(\"?\" + str(x) for x in v),\n",
      "                   \" \".join(initBindings[x].n3() for x in v))\n",
      "\n",
      "        self.resetQuery()\n",
      "        if self._is_contextual(queryGraph):\n",
      "            self.addDefaultGraph(queryGraph)\n",
      "        self.setQuery(query)\n",
      "\n",
      "        return Result.parse(SPARQLWrapper.query(self).response)\n",
      "\n",
      "    def triples(self, (s, p, o), context=None):\n",
      "        \"\"\"\n",
      "        - tuple **(s, o, p)**\n",
      "            the triple used as filter for the SPARQL select.\n",
      "            (None, None, None) means anything.\n",
      "        - context **context**\n",
      "            the graph effectively calling this method.\n",
      "\n",
      "        Returns a tuple of triples executing essentially a SPARQL like\n",
      "        SELECT ?subj ?pred ?obj WHERE { ?subj ?pred ?obj }\n",
      "\n",
      "        **context** may include three parameter\n",
      "        to refine the underlying query:\n",
      "         * LIMIT: an integer to limit the number of results\n",
      "         * OFFSET: an integer to enable paging of results\n",
      "         * ORDERBY: an instance of Variable('s'), Variable('o') or Variable('p')\n",
      "        or, by default, the first 'None' from the given triple\n",
      "\n",
      "        .. warning::\n",
      "        - Using LIMIT or OFFSET automatically include ORDERBY otherwise this is\n",
      "        because the results are retrieved in a not deterministic way (depends on\n",
      "        the walking path on the graph)\n",
      "        - Using OFFSET without defining LIMIT will discard the first OFFSET - 1\n",
      "        results\n",
      "\n",
      "        ``\n",
      "        a_graph.LIMIT = limit\n",
      "        a_graph.OFFSET = offset\n",
      "        triple_generator = a_graph.triples(mytriple):\n",
      "            #do something\n",
      "        #Removes LIMIT and OFFSET if not required for the next triple() calls\n",
      "        del a_graph.LIMIT\n",
      "        del a_graph.OFFSET\n",
      "        ``\n",
      "        \"\"\"\n",
      "\n",
      "        if ( isinstance(s, BNode) or\n",
      "             isinstance(p, BNode) or\n",
      "             isinstance(o, BNode) ):\n",
      "            raise Exception(\"SPARQLStore does not support Bnodes! \"\n",
      "                            \"See http://www.w3.org/TR/sparql11-query/#BGPsparqlBNodes\")\n",
      "\n",
      "        vars = []\n",
      "        if not s:\n",
      "            s = Variable('s')\n",
      "            vars.append(s)\n",
      "\n",
      "        if not p:\n",
      "            p = Variable('p')\n",
      "            vars.append(p)\n",
      "        if not o:\n",
      "            o = Variable('o')\n",
      "            vars.append(o)\n",
      "\n",
      "        if vars:\n",
      "            v = ' '.join([term.n3() for term in vars])\n",
      "        else:\n",
      "            v = '*'\n",
      "\n",
      "        query = \"SELECT %s WHERE { %s %s %s }\" % \\\n",
      "            (v, s.n3(), p.n3(), o.n3())\n",
      "\n",
      "        # The ORDER BY is necessary\n",
      "        if hasattr(context, LIMIT) or hasattr(context, OFFSET) \\\n",
      "            or hasattr(context, ORDERBY):\n",
      "            var = None\n",
      "            if isinstance(s, Variable):\n",
      "                var = s\n",
      "            elif isinstance(p, Variable):\n",
      "                var = p\n",
      "            elif isinstance(o, Variable):\n",
      "                var = o\n",
      "            elif hasattr(context, ORDERBY) \\\n",
      "                    and isinstance(getattr(context, ORDERBY), Variable):\n",
      "                var = getattr(context, ORDERBY)\n",
      "            query = query + ' %s %s' % (ORDERBY, var.n3())\n",
      "\n",
      "        try:\n",
      "            query = query + ' LIMIT %s' % int(getattr(context, LIMIT))\n",
      "        except (ValueError, TypeError, AttributeError):\n",
      "            pass\n",
      "        try:\n",
      "            query = query + ' OFFSET %s' % int(getattr(context, OFFSET))\n",
      "        except (ValueError, TypeError, AttributeError):\n",
      "            pass\n",
      "\n",
      "        self.resetQuery()\n",
      "        if self._is_contextual(context):\n",
      "            self.addDefaultGraph(context.identifier)\n",
      "        self.setQuery(query)\n",
      "\n",
      "        doc = ElementTree.parse(SPARQLWrapper.query(self).response)\n",
      "        # ElementTree.dump(doc)\n",
      "        for rt, vars in TraverseSPARQLResultDOM(doc, asDictionary=True):\n",
      "            yield (rt.get(s, s),\n",
      "                   rt.get(p, p),\n",
      "                   rt.get(o, o)), None\n",
      "\n",
      "    def triples_choices(self, (subject, predicate, object_), context=None):\n",
      "        \"\"\"\n",
      "        A variant of triples that can take a list of terms instead of a\n",
      "        single term in any slot.  Stores can implement this to optimize\n",
      "        the response time from the import default 'fallback' implementation,\n",
      "        which will iterate over each term in the list and dispatch to\n",
      "        triples.\n",
      "        \"\"\"\n",
      "        raise NotImplementedError('Triples choices currently not supported')\n",
      "\n",
      "    def __len__(self, context=None):\n",
      "        if not self.sparql11:\n",
      "            raise NotImplementedError(\n",
      "                \"For performance reasons, this is not\" +\n",
      "                \"supported for sparql1.0 endpoints\")\n",
      "        else:\n",
      "            self.resetQuery()\n",
      "            q = \"SELECT (count(*) as ?c) WHERE {?s ?p ?o .}\"\n",
      "            if self._is_contextual(context):\n",
      "                self.addDefaultGraph(context.identifier)\n",
      "            self.setQuery(q)\n",
      "            doc = ElementTree.parse(SPARQLWrapper.query(self).response)\n",
      "            rt, vars = iter(\n",
      "                TraverseSPARQLResultDOM(doc, asDictionary=True)).next()\n",
      "            return int(rt.get(Variable(\"c\")))\n",
      "\n",
      "    def contexts(self, triple=None):\n",
      "        \"\"\"\n",
      "        Iterates over results to \"SELECT ?NAME { GRAPH ?NAME { ?s ?p ?o } }\"\n",
      "        or \"SELECT ?NAME { GRAPH ?NAME {} }\" if triple is `None`.\n",
      "\n",
      "        Returns instances of this store with the SPARQL wrapper\n",
      "        object updated via addNamedGraph(?NAME).\n",
      "\n",
      "        This causes a named-graph-uri key / value  pair to be sent over\n",
      "        the protocol.\n",
      "\n",
      "        Please note that some SPARQL endpoints are not able to find empty named\n",
      "        graphs.\n",
      "        \"\"\"\n",
      "        self.resetQuery()\n",
      "\n",
      "        if triple:\n",
      "            s, p, o = triple\n",
      "            params = ((s if s else Variable('s')).n3(),\n",
      "                      (p if p else Variable('p')).n3(),\n",
      "                      (o if o else Variable('o')).n3())\n",
      "            self.setQuery('SELECT ?name WHERE { GRAPH ?name { %s %s %s }}' % params)\n",
      "        else:\n",
      "            self.setQuery('SELECT ?name WHERE { GRAPH ?name {} }')\n",
      "\n",
      "        doc = ElementTree.parse(SPARQLWrapper.query(self).response)\n",
      "\n",
      "        return (rt.get(Variable(\"name\"))\n",
      "                for rt, vars in TraverseSPARQLResultDOM(doc, asDictionary=True))\n",
      "\n",
      "    # Namespace persistence interface implementation\n",
      "    def bind(self, prefix, namespace):\n",
      "        self.nsBindings[prefix] = namespace\n",
      "\n",
      "    def prefix(self, namespace):\n",
      "        \"\"\" \"\"\"\n",
      "        return dict(\n",
      "            [(v, k) for k, v in self.nsBindings.items()]\n",
      "        ).get(namespace)\n",
      "\n",
      "    def namespace(self, prefix):\n",
      "        return self.nsBindings.get(prefix)\n",
      "\n",
      "    def namespaces(self):\n",
      "        for prefix, ns in self.nsBindings.items():\n",
      "            yield prefix, ns\n",
      "\n",
      "    def add_graph(self, graph):\n",
      "        raise TypeError('The SPARQL store is read only')\n",
      "\n",
      "    def remove_graph(self, graph):\n",
      "        raise TypeError('The SPARQL store is read only')\n",
      "\n",
      "    def _is_contextual(self, graph):\n",
      "        \"\"\" Returns `True` if the \"GRAPH\" keyword must appear\n",
      "        in the final SPARQL query sent to the endpoint.\n",
      "        \"\"\"\n",
      "        if (not self.context_aware) or (graph is None):\n",
      "            return False\n",
      "        if isinstance(graph, basestring):\n",
      "            return graph != '__UNION__'\n",
      "        else:\n",
      "            return graph.identifier != DATASET_DEFAULT_GRAPH_ID\n",
      "\n",
      "\n",
      "class SPARQLUpdateStore(SPARQLStore):\n",
      "    \"\"\"A store using SPARQL queries for reading and SPARQL Update for changes.\n",
      "\n",
      "    This can be context-aware, if so, any changes will be to the given named\n",
      "    graph only.\n",
      "\n",
      "    In favor of the SPARQL 1.1 motivated Dataset, we advise against using this\n",
      "    with ConjunctiveGraphs, as it reads and writes from and to the\n",
      "    \"default graph\". Exactly what this means depends on the endpoint and can\n",
      "    result in confusion.\n",
      "\n",
      "    For Graph objects, everything works as expected.\n",
      "\n",
      "    .. warning:: The SPARQL Update Store does not support blank-nodes!\n",
      "\n",
      "                 As blank-nodes acts as variables in SPARQL queries\n",
      "                 there is no way to query for a particular blank node.\n",
      "\n",
      "                 See http://www.w3.org/TR/sparql11-query/#BGPsparqlBNodes\n",
      "\n",
      "\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    where_pattern = re.compile(r\"\"\"(?P<where>WHERE\\s*{)\"\"\", re.IGNORECASE)\n",
      "\n",
      "    ##################################################################\n",
      "    ### Regex for injecting GRAPH blocks into updates on a context ###\n",
      "    ##################################################################\n",
      "\n",
      "    # Observations on the SPARQL grammar (http://www.w3.org/TR/2013/REC-sparql11-query-20130321/):\n",
      "    # 1. Only the terminals STRING_LITERAL1, STRING_LITERAL2,\n",
      "    #    STRING_LITERAL_LONG1, STRING_LITERAL_LONG2, and comments can contain\n",
      "    #    curly braces.\n",
      "    # 2. The non-terminals introduce curly braces in pairs only.\n",
      "    # 3. Unescaped \" can occur only in strings and comments.\n",
      "    # 3. Unescaped ' can occur only in strings, comments, and IRIRefs.\n",
      "    # 4. \\ always escapes the following character, especially \\\", \\', and\n",
      "    #    \\\\ denote literal \", ', and \\ respectively.\n",
      "    # 5. # always starts a comment outside of string and IRI\n",
      "    # 6. A comment ends at the next newline\n",
      "    # 7. IRIREFs need to be detected, as they may contain # without starting a comment\n",
      "    # 8. PrefixedNames do not contain a #\n",
      "    # As a consequence, it should be rather easy to detect strings and comments\n",
      "    # in order to avoid unbalanced curly braces.\n",
      "\n",
      "    # From the SPARQL grammar\n",
      "    STRING_LITERAL1 = ur\"'([^'\\\\]|\\\\.)*'\"\n",
      "    STRING_LITERAL2 = ur'\"([^\"\\\\]|\\\\.)*\"'\n",
      "    STRING_LITERAL_LONG1 = ur\"'''(('|'')?([^'\\\\]|\\\\.))*'''\"\n",
      "    STRING_LITERAL_LONG2 = ur'\"\"\"((\"|\"\")?([^\"\\\\]|\\\\.))*\"\"\"'\n",
      "    String = u'(%s)|(%s)|(%s)|(%s)' % (STRING_LITERAL1, STRING_LITERAL2, STRING_LITERAL_LONG1, STRING_LITERAL_LONG2)\n",
      "    IRIREF = ur'<([^<>\"{}|^`\\]\\\\\\[\\x00-\\x20])*>'\n",
      "    COMMENT = ur'#[^\\x0D\\x0A]*([\\x0D\\x0A]|\\Z)'\n",
      "\n",
      "    # Simplified grammar to find { at beginning and } at end of blocks\n",
      "    BLOCK_START = u'{'\n",
      "    BLOCK_END = u'}'\n",
      "    ESCAPED = ur'\\\\.'\n",
      "\n",
      "    # Match anything that doesn't start or end a block:\n",
      "    BlockContent = u'(%s)|(%s)|(%s)|(%s)' % (String, IRIREF, COMMENT, ESCAPED)\n",
      "    BlockFinding = u'(?P<block_start>%s)|(?P<block_end>%s)|(?P<block_content>%s)' % (BLOCK_START, BLOCK_END, BlockContent)\n",
      "    BLOCK_FINDING_PATTERN = re.compile(BlockFinding)\n",
      "\n",
      "    # Note that BLOCK_FINDING_PATTERN.finditer() will not cover the whole\n",
      "    # string with matches. Everything that is not matched will have to be\n",
      "    # part of the modified query as is.\n",
      "\n",
      "    ##################################################################\n",
      "\n",
      "\n",
      "    def __init__(self,\n",
      "                 queryEndpoint=None, update_endpoint=None,\n",
      "                 bNodeAsURI=False, sparql11=True,\n",
      "                 context_aware=True,\n",
      "                 postAsEncoded=True, autocommit=True):\n",
      "\n",
      "        SPARQLStore.__init__(self,\n",
      "                             queryEndpoint, bNodeAsURI, sparql11, context_aware, updateEndpoint=update_endpoint)\n",
      "\n",
      "        self.postAsEncoded = postAsEncoded\n",
      "        self.autocommit = autocommit\n",
      "        self._edits = None\n",
      "\n",
      "    def query(self,*args, **kwargs):\n",
      "        if not self.autocommit:\n",
      "            self.commit()\n",
      "        return SPARQLStore.query(self,*args, **kwargs)\n",
      "\t\n",
      "    def triples(self,*args, **kwargs):\n",
      "        if not self.autocommit:\n",
      "            self.commit()\n",
      "        return SPARQLStore.triples(self,*args, **kwargs)\n",
      "\t\n",
      "    def contexts(self,*args, **kwargs):\n",
      "        if not self.autocommit:\n",
      "            self.commit()\n",
      "        return SPARQLStore.contexts(self,*args, **kwargs)\n",
      "\t\n",
      "    def __len__(self,*args, **kwargs):\n",
      "        if not self.autocommit:\n",
      "            self.commit()\n",
      "        return SPARQLStore.__len__(self,*args, **kwargs)\n",
      "\n",
      "    def open(self, configuration, create=False):\n",
      "        \"\"\"\n",
      "        sets the endpoint URLs for this SPARQLStore\n",
      "        :param configuration: either a tuple of (queryEndpoint, update_endpoint),\n",
      "            or a string with the query endpoint\n",
      "        :param create: if True an exception is thrown.\n",
      "        \"\"\"\n",
      "\n",
      "        if create:\n",
      "            raise Exception(\"Cannot create a SPARQL Endpoint\")\n",
      "\n",
      "        if isinstance(configuration, tuple):\n",
      "            self.endpoint = configuration[0]\n",
      "            if len(configuration) > 1:\n",
      "                self.updateEndpoint = configuration[1]\n",
      "        else:\n",
      "            self.endpoint = configuration\n",
      "\n",
      "        if not self.updateEndpoint:\n",
      "            self.updateEndpoint = self.endpoint\n",
      "\n",
      "    def _transaction(self):\n",
      "        if self._edits == None:\n",
      "            self._edits = []\n",
      "        return self._edits\n",
      "\n",
      "    def __set_update_endpoint(self, update_endpoint):\n",
      "        self.updateEndpoint = update_endpoint\n",
      "\n",
      "    def __get_update_endpoint(self):\n",
      "        return self.updateEndpoint\n",
      "\n",
      "    update_endpoint = property(\n",
      "        __get_update_endpoint,\n",
      "        __set_update_endpoint,\n",
      "        doc='the HTTP URL for the Update endpoint, typically' +\n",
      "            'something like http://server/dataset/update')\n",
      "\n",
      "    # Transactional interfaces\n",
      "    def commit(self):\n",
      "        \"\"\" add(), addN(), and remove() are transactional to reduce overhead of many small edits. \n",
      "            Read and update() calls will automatically commit any outstanding edits. \n",
      "            This should behave as expected most of the time, except that alternating writes \n",
      "            and reads can degenerate to the original call-per-triple situation that originally existed.\n",
      "        \"\"\"\n",
      "        if self._edits and len(self._edits) > 0:\n",
      "            r = self._do_update('\\n;\\n'.join(self._edits))\n",
      "            self._edits = None\n",
      "            return r\n",
      "\n",
      "    def rollback(self):\n",
      "        self._edits = None\n",
      "\n",
      "    def add(self, spo, context=None, quoted=False):\n",
      "        \"\"\" Add a triple to the store of triples. \"\"\"\n",
      "\n",
      "        if not self.endpoint:\n",
      "            raise Exception(\"UpdateEndpoint is not set - call 'open'\")\n",
      "\n",
      "        assert not quoted\n",
      "        (subject, predicate, obj) = spo\n",
      "\n",
      "        if ( isinstance(subject, BNode) or\n",
      "             isinstance(predicate, BNode) or\n",
      "             isinstance(obj, BNode) ):\n",
      "            raise Exception(\"SPARQLStore does not support Bnodes! \"\n",
      "                            \"See http://www.w3.org/TR/sparql11-query/#BGPsparqlBNodes\")\n",
      "\n",
      "\n",
      "        triple = \"%s %s %s .\" % (subject.n3(), predicate.n3(), obj.n3())\n",
      "        if self._is_contextual(context):\n",
      "            q = \"INSERT DATA { GRAPH %s { %s } }\" % (\n",
      "                context.identifier.n3(), triple)\n",
      "        else:\n",
      "            q = \"INSERT DATA { %s }\" % triple\n",
      "        self._transaction().append(q)\n",
      "        if self.autocommit:\n",
      "            self.commit()\n",
      "\n",
      "    def addN(self, quads):\n",
      "        \"\"\" Add a list of quads to the store. \"\"\"\n",
      "        if not self.endpoint:\n",
      "            raise Exception(\"UpdateEndpoint is not set - call 'open'\")\n",
      "\n",
      "        contexts = collections.defaultdict(list)\n",
      "        for subject, predicate, obj, context in quads:\n",
      "            contexts[context].append((subject,predicate,obj))\n",
      "        data = []\n",
      "        for context in contexts:\n",
      "            triples = [\"%s %s %s .\" % (x[0].n3(), x[1].n3(), x[2].n3()) for x in contexts[context]]\n",
      "            data.append(\"INSERT DATA { GRAPH <%s> { %s } }\\n\" % (context.identifier, '\\n'.join(triples)))\n",
      "        self._transaction().extend(data)\n",
      "        if self.autocommit:\n",
      "            self.commit()\n",
      "\n",
      "    def remove(self, spo, context):\n",
      "        \"\"\" Remove a triple from the store \"\"\"\n",
      "        if not self.endpoint:\n",
      "            raise Exception(\"UpdateEndpoint is not set - call 'open'\")\n",
      "\n",
      "        (subject, predicate, obj) = spo\n",
      "        if not subject:\n",
      "            subject = Variable(\"S\")\n",
      "        if not predicate:\n",
      "            predicate = Variable(\"P\")\n",
      "        if not obj:\n",
      "            obj = Variable(\"O\")\n",
      "\n",
      "        triple = \"%s %s %s .\" % (subject.n3(), predicate.n3(), obj.n3())\n",
      "        if self._is_contextual(context):\n",
      "            q = \"DELETE { GRAPH %s { %s } } WHERE { GRAPH %s { %s } }\" % (\n",
      "                context.identifier.n3(), triple,\n",
      "                context.identifier.n3(), triple)\n",
      "        else:\n",
      "            q = \"DELETE { %s } WHERE { %s } \" % (triple, triple)\n",
      "        self._transaction().append(q)\n",
      "        if self.autocommit:\n",
      "            self.commit()\n",
      "\n",
      "    def _do_update(self, update):\n",
      "        self.resetQuery()\n",
      "        self.setQuery(update)\n",
      "        self.setMethod(POST)\n",
      "        self.setRequestMethod(URLENCODED if self.postAsEncoded else POSTDIRECTLY)\n",
      "\n",
      "        result = SPARQLWrapper.query(self)\n",
      "        return result\n",
      "\n",
      "    def update(self, query,\n",
      "               initNs={},\n",
      "               initBindings={},\n",
      "               queryGraph=None,\n",
      "               DEBUG=False):\n",
      "        \"\"\"\n",
      "        Perform a SPARQL Update Query against the endpoint,\n",
      "        INSERT, LOAD, DELETE etc.\n",
      "        Setting initNs adds PREFIX declarations to the beginning of\n",
      "        the update. Setting initBindings adds inline VALUEs to the\n",
      "        beginning of every WHERE clause. By the SPARQL grammar, all\n",
      "        operations that support variables (namely INSERT and DELETE)\n",
      "        require a WHERE clause.\n",
      "        Important: initBindings fails if the update contains the\n",
      "        substring 'WHERE {' which does not denote a WHERE clause, e.g.\n",
      "        if it is part of a literal.\n",
      "\n",
      "        .. admonition:: Context-aware query rewriting\n",
      "\n",
      "            - **When:**  If context-awareness is enabled and the graph is not the default graph of the store.\n",
      "            - **Why:** To ensure consistency with the :class:`~rdflib.plugins.memory.IOMemory` store.\n",
      "              The graph must except \"local\" SPARQL requests (requests with no GRAPH keyword)\n",
      "              like if it was the default graph.\n",
      "            - **What is done:** These \"local\" queries are rewritten by this store.\n",
      "              The content of each block of a SPARQL Update operation is wrapped in a GRAPH block\n",
      "              except if the block is empty.\n",
      "              This basically causes INSERT, INSERT DATA, DELETE, DELETE DATA and WHERE to operate\n",
      "              only on the context.\n",
      "            - **Example:** `\"INSERT DATA { <urn:michel> <urn:likes> <urn:pizza> }\"` is converted into\n",
      "              `\"INSERT DATA { GRAPH <urn:graph> { <urn:michel> <urn:likes> <urn:pizza> } }\"`.\n",
      "            - **Warning:** Queries are presumed to be \"local\" but this assumption is **not checked**.\n",
      "              For instance, if the query already contains GRAPH blocks, the latter will be wrapped in new GRAPH blocks.\n",
      "            - **Warning:** A simplified grammar is used that should tolerate\n",
      "              extensions of the SPARQL grammar. Still, the process may fail in\n",
      "              uncommon situations and produce invalid output.\n",
      "\n",
      "        \"\"\"\n",
      "        if not self.endpoint:\n",
      "            raise Exception(\"UpdateEndpoint is not set - call 'open'\")\n",
      "\n",
      "        self.debug = DEBUG\n",
      "        assert isinstance(query, basestring)\n",
      "        self.setNamespaceBindings(initNs)\n",
      "        query = self.injectPrefixes(query)\n",
      "\n",
      "        if self._is_contextual(queryGraph):\n",
      "            query = self._insert_named_graph(query, queryGraph)\n",
      "\n",
      "        if initBindings:\n",
      "            # For INSERT and DELETE the WHERE clause is obligatory\n",
      "            # (http://www.w3.org/TR/2013/REC-sparql11-query-20130321/#rModify)\n",
      "            # Other query types do not allow variables and don't\n",
      "            # have a WHERE clause.  This also works for updates with\n",
      "            # more than one INSERT/DELETE.\n",
      "            v = list(initBindings)\n",
      "            values = \"\\nVALUES ( %s )\\n{ ( %s ) }\\n\"\\\n",
      "                % (\" \".join(\"?\" + str(x) for x in v),\n",
      "                   \" \".join(initBindings[x].n3() for x in v))\n",
      "\n",
      "            query = self.where_pattern.sub(\"WHERE { \" + values, query)\n",
      "\n",
      "        self._transaction().append(query)\n",
      "        if self.autocommit:\n",
      "            self.commit()\n",
      "\n",
      "    def _insert_named_graph(self, query, query_graph):\n",
      "        \"\"\"\n",
      "            Inserts GRAPH <query_graph> {} into blocks of SPARQL Update operations\n",
      "\n",
      "            For instance,  \"INSERT DATA { <urn:michel> <urn:likes> <urn:pizza> }\"\n",
      "            is converted into\n",
      "            \"INSERT DATA { GRAPH <urn:graph> { <urn:michel> <urn:likes> <urn:pizza> } }\"\n",
      "        \"\"\"\n",
      "        graph_block_open = \" GRAPH <%s> {\" % query_graph\n",
      "        graph_block_close = \"} \"\n",
      "\n",
      "        # SPARQL Update supports the following operations:\n",
      "        # LOAD, CLEAR, DROP, ADD, MOVE, COPY, CREATE, INSERT DATA, DELETE DATA, DELETE/INSERT, DELETE WHERE\n",
      "        # LOAD, CLEAR, DROP, ADD, MOVE, COPY, CREATE do not make much sense in a context.\n",
      "        # INSERT DATA, DELETE DATA, and DELETE WHERE require the contents of their block to be wrapped in a GRAPH <?> { }.\n",
      "        # DELETE/INSERT supports the WITH keyword, which sets the graph to be\n",
      "        # used for all following DELETE/INSERT instruction including the\n",
      "        # non-optional WHERE block. Equivalently, a GRAPH block can be added to\n",
      "        # all blocks.\n",
      "        #\n",
      "        # Strategy employed here: Wrap the contents of every top-level block into a `GRAPH <?> { }`.\n",
      "\n",
      "        level = 0\n",
      "        modified_query = []\n",
      "        pos = 0\n",
      "        for match in self.BLOCK_FINDING_PATTERN.finditer(query):\n",
      "            if match.group('block_start') is not None:\n",
      "                level += 1\n",
      "                if level == 1:\n",
      "                    modified_query.append(query[pos:match.end()])\n",
      "                    modified_query.append(graph_block_open)\n",
      "                    pos = match.end()\n",
      "            elif match.group('block_end') is not None:\n",
      "                if level == 1:\n",
      "                    since_previous_pos = query[pos:match.start()]\n",
      "                    if modified_query[-1] is graph_block_open and (since_previous_pos == \"\" or since_previous_pos.isspace()):\n",
      "                        # In this case, adding graph_block_start and\n",
      "                        # graph_block_end results in an empty GRAPH block. Some\n",
      "                        # enpoints (e.g. TDB) can not handle this. Therefore\n",
      "                        # remove the previously added block_start.\n",
      "                        modified_query.pop()\n",
      "                        modified_query.append(since_previous_pos)\n",
      "                    else:\n",
      "                        modified_query.append(since_previous_pos)\n",
      "                        modified_query.append(graph_block_close)\n",
      "                    pos = match.start()\n",
      "                level -= 1\n",
      "        modified_query.append(query[pos:])\n",
      "\n",
      "        return \"\".join(modified_query)\n",
      "\n",
      "    def add_graph(self, graph):\n",
      "        if not self.graph_aware:\n",
      "            Store.add_graph(self, graph)\n",
      "        elif graph.identifier != DATASET_DEFAULT_GRAPH_ID:\n",
      "            self.update(\"CREATE GRAPH <%s>\" % graph.identifier)\n",
      "\n",
      "    def remove_graph(self, graph):\n",
      "        if not self.graph_aware:\n",
      "            Store.remove_graph(self, graph)\n",
      "        elif graph.identifier == DATASET_DEFAULT_GRAPH_ID:\n",
      "            self.update(\"DROP DEFAULT\")\n",
      "        else:\n",
      "            self.update(\"DROP GRAPH <%s>\" % graph.identifier)\n",
      "\n",
      "['*', '*', '*', '*', '*', '*', '*', '*', '*', '*']\n",
      "#!/usr/bin/env python\n",
      "\n",
      "import os, sys, json\n",
      "from common_paths import *\n",
      "import spec_validator\n",
      "import argparse\n",
      "\n",
      "\n",
      "def expand_pattern(expansion_pattern, test_expansion_schema):\n",
      "    expansion = {}\n",
      "    for artifact_key in expansion_pattern:\n",
      "        artifact_value = expansion_pattern[artifact_key]\n",
      "        if artifact_value == '*':\n",
      "            expansion[artifact_key] = test_expansion_schema[artifact_key]\n",
      "        elif isinstance(artifact_value, list):\n",
      "            expansion[artifact_key] = artifact_value\n",
      "        elif isinstance(artifact_value, dict):\n",
      "            # Flattened expansion.\n",
      "            expansion[artifact_key] = []\n",
      "            values_dict = expand_pattern(artifact_value,\n",
      "                                         test_expansion_schema[artifact_key])\n",
      "            for sub_key in values_dict.keys():\n",
      "                expansion[artifact_key] += values_dict[sub_key]\n",
      "        else:\n",
      "            expansion[artifact_key] = [artifact_value]\n",
      "\n",
      "    return expansion\n",
      "\n",
      "\n",
      "def permute_expansion(expansion, artifact_order, selection = {}, artifact_index = 0):\n",
      "    assert isinstance(artifact_order, list), \"artifact_order should be a list\"\n",
      "\n",
      "    if artifact_index >= len(artifact_order):\n",
      "        yield selection\n",
      "        return\n",
      "\n",
      "    artifact_key = artifact_order[artifact_index]\n",
      "\n",
      "    for artifact_value in expansion[artifact_key]:\n",
      "        selection[artifact_key] = artifact_value\n",
      "        for next_selection in permute_expansion(expansion,\n",
      "                                                artifact_order,\n",
      "                                                selection,\n",
      "                                                artifact_index + 1):\n",
      "            yield next_selection\n",
      "\n",
      "\n",
      "def generate_selection(selection, spec, test_html_template_basename):\n",
      "    selection['spec_name'] = spec['name']\n",
      "    selection['spec_title'] = spec['title']\n",
      "    selection['spec_description'] = spec['description']\n",
      "    selection['spec_specification_url'] = spec['specification_url']\n",
      "\n",
      "    test_filename = test_file_path_pattern % selection\n",
      "    test_headers_filename = test_filename + \".headers\"\n",
      "    test_directory = os.path.dirname(test_filename)\n",
      "    full_path = os.path.join(spec_directory, test_directory)\n",
      "\n",
      "    test_html_template = get_template(test_html_template_basename)\n",
      "    test_js_template = get_template(\"test.js.template\")\n",
      "    disclaimer_template = get_template('disclaimer.template')\n",
      "    test_description_template = get_template(\"test_description.template\")\n",
      "\n",
      "    html_template_filename = os.path.join(template_directory,\n",
      "                                          test_html_template_basename)\n",
      "    generated_disclaimer = disclaimer_template \\\n",
      "        % {'generating_script_filename': os.path.relpath(__file__,\n",
      "           test_root_directory),\n",
      "           'html_template_filename': os.path.relpath(html_template_filename,\n",
      "           test_root_directory)}\n",
      "\n",
      "    selection['generated_disclaimer'] = generated_disclaimer.rstrip()\n",
      "    test_description_template = \\\n",
      "        test_description_template.rstrip().replace(\"\\n\", \"\\n\" + \" \" * 33)\n",
      "    selection['test_description'] = test_description_template % selection\n",
      "\n",
      "    # Adjust the template for the test invoking JS. Indent it to look nice.\n",
      "    indent = \"\\n\" + \" \" * 6;\n",
      "    test_js_template = indent + test_js_template.replace(\"\\n\", indent);\n",
      "    selection['test_js'] = test_js_template % selection\n",
      "\n",
      "    # Directory for the test files.\n",
      "    try:\n",
      "        os.makedirs(full_path)\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    # TODO(kristijanburnik): Implement the opt-in-method here.\n",
      "    opt_in_method = selection['opt_in_method']\n",
      "    selection['meta_opt_in'] = ''\n",
      "    if opt_in_method == 'meta-csp':\n",
      "        selection['meta_opt_in'] = '<meta http-equiv=\"Content-Security-Policy\" ' + \\\n",
      "                                   'content=\"block-all-mixed-content\">'\n",
      "    elif opt_in_method == 'http-csp':\n",
      "        opt_in_headers = \"Content-Security-Policy: block-all-mixed-content\\n\"\n",
      "        write_file(test_headers_filename, opt_in_headers)\n",
      "    elif opt_in_method == 'no-opt-in':\n",
      "        pass\n",
      "    else:\n",
      "        raise ValueError(\"Invalid opt_in_method %s\" % opt_in_method)\n",
      "\n",
      "    # Write out the generated HTML file.\n",
      "    write_file(test_filename, test_html_template % selection)\n",
      "\n",
      "def generate_test_source_files(spec_json, target):\n",
      "    test_expansion_schema = spec_json['test_expansion_schema']\n",
      "    specification = spec_json['specification']\n",
      "\n",
      "    spec_json_js_template = get_template('spec_json.js.template')\n",
      "    write_file(generated_spec_json_filename,\n",
      "               spec_json_js_template % {'spec_json': json.dumps(spec_json)})\n",
      "\n",
      "    # Choose a debug/release template depending on the target.\n",
      "    html_template = \"test.%s.html.template\" % target\n",
      "\n",
      "    artifact_order = test_expansion_schema.keys() + ['name']\n",
      "\n",
      "    # Create list of excluded tests.\n",
      "    exclusion_dict = {}\n",
      "    for excluded_pattern in spec_json['excluded_tests']:\n",
      "        excluded_expansion = \\\n",
      "            expand_pattern(excluded_pattern,\n",
      "                                          test_expansion_schema)\n",
      "        for excluded_selection in permute_expansion(excluded_expansion, artifact_order):\n",
      "            excluded_selection_path = selection_pattern % excluded_selection\n",
      "            exclusion_dict[excluded_selection_path] = True\n",
      "\n",
      "    for spec in specification:\n",
      "        for expansion_pattern in spec['test_expansion']:\n",
      "            expansion = expand_pattern(expansion_pattern,\n",
      "                                                      test_expansion_schema)\n",
      "            for selection in permute_expansion(expansion, artifact_order):\n",
      "                selection_path = selection_pattern % selection\n",
      "                if not selection_path in exclusion_dict:\n",
      "                    generate_selection(selection,\n",
      "                                       spec,\n",
      "                                       html_template)\n",
      "                else:\n",
      "                    print 'Excluding selection:', selection_path\n",
      "\n",
      "\n",
      "def main(target, spec_filename):\n",
      "    spec_json = load_spec_json(spec_filename);\n",
      "    spec_validator.assert_valid_spec_json(spec_json)\n",
      "    generate_test_source_files(spec_json, target)\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    parser = argparse.ArgumentParser(description='Test suite generator utility')\n",
      "    parser.add_argument('-t', '--target', type = str,\n",
      "        choices = (\"release\", \"debug\"), default = \"release\",\n",
      "        help = 'Sets the appropriate template for generating tests')\n",
      "    parser.add_argument('-s', '--spec', type = str, default = None,\n",
      "        help = 'Specify a file used for describing and generating the tests')\n",
      "    # TODO(kristijanburnik): Add option for the spec_json file.\n",
      "    args = parser.parse_args()\n",
      "    main(args.target, args.spec)\n",
      "\n",
      "['*', '*', '*', '*', '*', '*', '*', '*', '*', '*']\n",
      "#!/usr/bin/env python\n",
      "\n",
      "import os, sys, json\n",
      "from common_paths import *\n",
      "import spec_validator\n",
      "import argparse\n",
      "\n",
      "\n",
      "def expand_pattern(expansion_pattern, test_expansion_schema):\n",
      "    expansion = {}\n",
      "    for artifact_key in expansion_pattern:\n",
      "        artifact_value = expansion_pattern[artifact_key]\n",
      "        if artifact_value == '*':\n",
      "            expansion[artifact_key] = test_expansion_schema[artifact_key]\n",
      "        elif isinstance(artifact_value, list):\n",
      "            expansion[artifact_key] = artifact_value\n",
      "        elif isinstance(artifact_value, dict):\n",
      "            # Flattened expansion.\n",
      "            expansion[artifact_key] = []\n",
      "            values_dict = expand_pattern(artifact_value,\n",
      "                                         test_expansion_schema[artifact_key])\n",
      "            for sub_key in values_dict.keys():\n",
      "                expansion[artifact_key] += values_dict[sub_key]\n",
      "        else:\n",
      "            expansion[artifact_key] = [artifact_value]\n",
      "\n",
      "    return expansion\n",
      "\n",
      "\n",
      "def permute_expansion(expansion, artifact_order, selection = {}, artifact_index = 0):\n",
      "    assert isinstance(artifact_order, list), \"artifact_order should be a list\"\n",
      "\n",
      "    if artifact_index >= len(artifact_order):\n",
      "        yield selection\n",
      "        return\n",
      "\n",
      "    artifact_key = artifact_order[artifact_index]\n",
      "\n",
      "    for artifact_value in expansion[artifact_key]:\n",
      "        selection[artifact_key] = artifact_value\n",
      "        for next_selection in permute_expansion(expansion,\n",
      "                                                artifact_order,\n",
      "                                                selection,\n",
      "                                                artifact_index + 1):\n",
      "            yield next_selection\n",
      "\n",
      "\n",
      "def generate_selection(selection, spec, test_html_template_basename):\n",
      "    selection['spec_name'] = spec['name']\n",
      "    selection['spec_title'] = spec['title']\n",
      "    selection['spec_description'] = spec['description']\n",
      "    selection['spec_specification_url'] = spec['specification_url']\n",
      "\n",
      "    test_filename = test_file_path_pattern % selection\n",
      "    test_headers_filename = test_filename + \".headers\"\n",
      "    test_directory = os.path.dirname(test_filename)\n",
      "    full_path = os.path.join(spec_directory, test_directory)\n",
      "\n",
      "    test_html_template = get_template(test_html_template_basename)\n",
      "    test_js_template = get_template(\"test.js.template\")\n",
      "    disclaimer_template = get_template('disclaimer.template')\n",
      "    test_description_template = get_template(\"test_description.template\")\n",
      "\n",
      "    html_template_filename = os.path.join(template_directory,\n",
      "                                          test_html_template_basename)\n",
      "    generated_disclaimer = disclaimer_template \\\n",
      "        % {'generating_script_filename': os.path.relpath(__file__,\n",
      "           test_root_directory),\n",
      "           'html_template_filename': os.path.relpath(html_template_filename,\n",
      "           test_root_directory)}\n",
      "\n",
      "    selection['generated_disclaimer'] = generated_disclaimer.rstrip()\n",
      "    test_description_template = \\\n",
      "        test_description_template.rstrip().replace(\"\\n\", \"\\n\" + \" \" * 33)\n",
      "    selection['test_description'] = test_description_template % selection\n",
      "\n",
      "    # Adjust the template for the test invoking JS. Indent it to look nice.\n",
      "    indent = \"\\n\" + \" \" * 6;\n",
      "    test_js_template = indent + test_js_template.replace(\"\\n\", indent);\n",
      "    selection['test_js'] = test_js_template % selection\n",
      "\n",
      "    # Directory for the test files.\n",
      "    try:\n",
      "        os.makedirs(full_path)\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    # TODO(kristijanburnik): Implement the opt-in-method here.\n",
      "    opt_in_method = selection['opt_in_method']\n",
      "    selection['meta_opt_in'] = ''\n",
      "    if opt_in_method == 'meta-csp':\n",
      "        selection['meta_opt_in'] = '<meta http-equiv=\"Content-Security-Policy\" ' + \\\n",
      "                                   'content=\"block-all-mixed-content\">'\n",
      "    elif opt_in_method == 'http-csp':\n",
      "        opt_in_headers = \"Content-Security-Policy: block-all-mixed-content\\n\"\n",
      "        write_file(test_headers_filename, opt_in_headers)\n",
      "    elif opt_in_method == 'no-opt-in':\n",
      "        pass\n",
      "    else:\n",
      "        raise ValueError(\"Invalid opt_in_method %s\" % opt_in_method)\n",
      "\n",
      "    # Write out the generated HTML file.\n",
      "    write_file(test_filename, test_html_template % selection)\n",
      "\n",
      "def generate_test_source_files(spec_json, target):\n",
      "    test_expansion_schema = spec_json['test_expansion_schema']\n",
      "    specification = spec_json['specification']\n",
      "\n",
      "    spec_json_js_template = get_template('spec_json.js.template')\n",
      "    write_file(generated_spec_json_filename,\n",
      "               spec_json_js_template % {'spec_json': json.dumps(spec_json)})\n",
      "\n",
      "    # Choose a debug/release template depending on the target.\n",
      "    html_template = \"test.%s.html.template\" % target\n",
      "\n",
      "    artifact_order = test_expansion_schema.keys() + ['name']\n",
      "\n",
      "    # Create list of excluded tests.\n",
      "    exclusion_dict = {}\n",
      "    for excluded_pattern in spec_json['excluded_tests']:\n",
      "        excluded_expansion = \\\n",
      "            expand_pattern(excluded_pattern,\n",
      "                                          test_expansion_schema)\n",
      "        for excluded_selection in permute_expansion(excluded_expansion, artifact_order):\n",
      "            excluded_selection_path = selection_pattern % excluded_selection\n",
      "            exclusion_dict[excluded_selection_path] = True\n",
      "\n",
      "    for spec in specification:\n",
      "        for expansion_pattern in spec['test_expansion']:\n",
      "            expansion = expand_pattern(expansion_pattern,\n",
      "                                                      test_expansion_schema)\n",
      "            for selection in permute_expansion(expansion, artifact_order):\n",
      "                selection_path = selection_pattern % selection\n",
      "                if not selection_path in exclusion_dict:\n",
      "                    generate_selection(selection,\n",
      "                                       spec,\n",
      "                                       html_template)\n",
      "                else:\n",
      "                    print 'Excluding selection:', selection_path\n",
      "\n",
      "\n",
      "def main(target, spec_filename):\n",
      "    spec_json = load_spec_json(spec_filename);\n",
      "    spec_validator.assert_valid_spec_json(spec_json)\n",
      "    generate_test_source_files(spec_json, target)\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    parser = argparse.ArgumentParser(description='Test suite generator utility')\n",
      "    parser.add_argument('-t', '--target', type = str,\n",
      "        choices = (\"release\", \"debug\"), default = \"release\",\n",
      "        help = 'Sets the appropriate template for generating tests')\n",
      "    parser.add_argument('-s', '--spec', type = str, default = None,\n",
      "        help = 'Specify a file used for describing and generating the tests')\n",
      "    # TODO(kristijanburnik): Add option for the spec_json file.\n",
      "    args = parser.parse_args()\n",
      "    main(args.target, args.spec)\n",
      "\n",
      "['*', '*', '*', '*', '*', '*', '*', '*', '*', '*']\n",
      "#  Copyright 2017 Intel Corporation\n",
      "#\n",
      "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "#  you may not use this file except in compliance with the License.\n",
      "#  You may obtain a copy of the License at\n",
      "#\n",
      "#       http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "#  Unless required by applicable law or agreed to in writing, software\n",
      "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "#  See the License for the specific language governing permissions and\n",
      "#  limitations under the License.\n",
      "\n",
      "from pathlib import Path\n",
      "from typing import Iterable, Sequence\n",
      "\n",
      "import nibabel as nib\n",
      "import numpy as np\n",
      "import pytest\n",
      "\n",
      "from brainiak import io\n",
      "\n",
      "\n",
      "@pytest.fixture\n",
      "def in_dir() -> Path:\n",
      "    return Path(__file__).parent / \"data\"\n",
      "\n",
      "\n",
      "@pytest.fixture\n",
      "def expected_image_data_shape() -> Sequence[int]:\n",
      "    return (64, 64, 26, 10)\n",
      "\n",
      "\n",
      "@pytest.fixture\n",
      "def mask_path(in_dir: Path) -> Path:\n",
      "    return in_dir / \"mask.nii.gz\"\n",
      "\n",
      "\n",
      "@pytest.fixture\n",
      "def labels_path(in_dir: Path) -> Path:\n",
      "    return in_dir / \"epoch_labels.npy\"\n",
      "\n",
      "\n",
      "@pytest.fixture\n",
      "def expected_condition_spec_shape() -> Sequence[int]:\n",
      "    return (2, 2, 10)\n",
      "\n",
      "\n",
      "@pytest.fixture\n",
      "def expected_n_subjects() -> int:\n",
      "    return 2\n",
      "\n",
      "\n",
      "@pytest.fixture\n",
      "def image_paths(in_dir: Path) -> Iterable[Path]:\n",
      "    return (in_dir / \"subject1_bet.nii.gz\", in_dir / \"subject2_bet.nii.gz\")\n",
      "\n",
      "\n",
      "def test_load_images_from_dir_data_shape(\n",
      "        in_dir: Path,\n",
      "        expected_image_data_shape: Sequence[int],\n",
      "        expected_n_subjects: int\n",
      "        ) -> None:\n",
      "    for i, image in enumerate(io.load_images_from_dir(in_dir, \"bet.nii.gz\")):\n",
      "        assert image.get_data().shape == (64, 64, 26, 10)\n",
      "    assert i + 1 == expected_n_subjects\n",
      "\n",
      "\n",
      "def test_load_images_data_shape(\n",
      "        image_paths: Iterable[Path],\n",
      "        expected_image_data_shape: Sequence[int],\n",
      "        expected_n_subjects: int\n",
      "        ) -> None:\n",
      "    for i, image in enumerate(io.load_images(image_paths)):\n",
      "        assert image.get_data().shape == (64, 64, 26, 10)\n",
      "    assert i + 1 == expected_n_subjects\n",
      "\n",
      "\n",
      "def test_load_boolean_mask(mask_path: Path) -> None:\n",
      "    mask = io.load_boolean_mask(mask_path)\n",
      "    assert mask.dtype == np.bool\n",
      "\n",
      "\n",
      "def test_load_boolean_mask_predicate(mask_path: Path) -> None:\n",
      "    mask = io.load_boolean_mask(mask_path, lambda x: np.logical_not(x))\n",
      "    expected_mask = np.logical_not(io.load_boolean_mask(mask_path))\n",
      "    assert np.array_equal(mask, expected_mask)\n",
      "\n",
      "\n",
      "def test_load_labels(labels_path: Path,\n",
      "                     expected_condition_spec_shape: Sequence[int],\n",
      "                     expected_n_subjects: int) -> None:\n",
      "    condition_specs = io.load_labels(labels_path)\n",
      "    i = 0\n",
      "    for condition_spec in condition_specs:\n",
      "        assert condition_spec.shape == expected_condition_spec_shape\n",
      "        i += 1\n",
      "    assert i == expected_n_subjects\n",
      "\n",
      "\n",
      "def test_save_as_nifti_file(tmpdir) -> None:\n",
      "    out_file = str(tmpdir / \"nifti.nii\")\n",
      "    shape = (4, 4, 4)\n",
      "    io.save_as_nifti_file(np.ones(shape), np.eye(4), out_file)\n",
      "    assert nib.load(out_file).get_data().shape == shape\n",
      "\n",
      "['*', '*', '*', '*', '*', '*', '*', '*', '*', '*']\n",
      "import os\n",
      "import sys\n",
      "import functools\n",
      "import concurrent.futures\n",
      "\n",
      "from Bio import SeqIO\n",
      "\n",
      "from kindel import kindel\n",
      "\n",
      "\n",
      "bwa_path = 'tests/data_bwa_mem/'\n",
      "seg_path = 'tests/data_segemehl/'\n",
      "mm2_path = 'tests/data_minimap2/'\n",
      "\n",
      "bwa_fns = [bwa_path + fn for fn in os.listdir(bwa_path) if fn.endswith('.bam')]\n",
      "seg_fns = [seg_path + fn for fn in os.listdir(seg_path) if fn.endswith('.bam')]\n",
      "mm2_fns = [mm2_path + fn for fn in os.listdir(mm2_path) if fn.endswith('am')]\n",
      "\n",
      "test_aln = list(kindel.parse_bam(bwa_path + '1.1.sub_test.bam').values())[0]\n",
      "\n",
      "\n",
      "# UNIT\n",
      "\n",
      "def test_consensus():\n",
      "    pos_weight = {'A': 1, 'C': 2, 'G': 3, 'T': 4, 'N': 5}\n",
      "    assert kindel.consensus(pos_weight)[0] == 'N'\n",
      "    assert kindel.consensus(pos_weight)[1] == 5\n",
      "    assert kindel.consensus(pos_weight)[2] == 0.33\n",
      "    assert kindel.consensus(pos_weight)[3] is False\n",
      "    pos_weight_tie = {'A': 5, 'C': 5, 'G': 3, 'T': 4, 'N': 1}\n",
      "    assert kindel.consensus(pos_weight_tie)[2]\n",
      "\n",
      "\n",
      "def test_merge_by_lcs():\n",
      "    one = ('AACTGCCGCTAGGGGCGCGTTCGGGCTCGCCAACATCTTCAGTCCGGG',\n",
      "           'GCCGCTAGGGGCGCGTTCGGGCTCGCCAACATCTTCAGTCCGGGCGCTAAGCAGAACA')\n",
      "    two = ('AACTGCCGCTAGGGGCGCGTTCGGGCTCGCCAACATCTTCAGTCCGGGCGCTAAGCAGAACATC',\n",
      "           'GCAGATACCTACACCACCGGGGGAACTGCCGCTAGGGGCGCGTTCGGGCTCGCCAACATCTTCAGTCCGGGCGCTAAGCAGAACA')\n",
      "    short = ('AT', 'CG')\n",
      "    assert kindel.merge_by_lcs(*one) == 'AACTGCCGCTAGGGGCGCGTTCGGGCTCGCCAACATCTTCAGTCCGGGCGCTAAGCAGAACA'\n",
      "    assert kindel.merge_by_lcs(*two) == 'AACTGCCGCTAGGGGCGCGTTCGGGCTCGCCAACATCTTCAGTCCGGGCGCTAAGCAGAACA'\n",
      "    assert kindel.merge_by_lcs(*short) == None\n",
      "\n",
      "\n",
      "# FUNCTIONAL\n",
      "\n",
      "def test_parse_bam():\n",
      "    assert test_aln.ref_id == 'ENA|EU155341|EU155341.2'\n",
      "    assert len(test_aln.weights) == 9306 \n",
      "\n",
      "\n",
      "def test_cdrp_consensuses():\n",
      "    cdrps = kindel.cdrp_consensuses(test_aln.weights, test_aln.clip_start_weights,\n",
      "                                    test_aln.clip_end_weights, test_aln.clip_start_depth,\n",
      "                                    test_aln.clip_end_depth, 0.1, 10)\n",
      "    print(cdrps)\n",
      "    assert cdrps[0][0].seq == 'AACTGCCGCTAGGGGCGCGTTCGGGCTCGCCAACATCTTCAGTCCGGGCGCTAAGCAGAACATCCAGCTGATCAACA'\n",
      "    assert cdrps[0][1].seq == 'AGCGTCGATGCAGATACCTACACCACCGGGGGAACTGCCGCTAGGGGCGCGTTCGGGCTCGCCAACATCTTCAGTCCGGGCGCTAAGCAGAACA'\n",
      "\n",
      "\n",
      "def test_bam_to_consensus_bwa():\n",
      "    for fn in bwa_fns:\n",
      "        assert kindel.bam_to_consensus(fn)\n",
      "\n",
      "\n",
      "def test_bam_to_consensus_minimap2():\n",
      "    for fn in mm2_fns:\n",
      "        assert kindel.bam_to_consensus(fn)\n",
      "\n",
      "\n",
      "def test_bam_to_consensus_realign_bwa():\n",
      "    for fn in bwa_fns:\n",
      "        assert kindel.bam_to_consensus(fn, realign=True)\n",
      "\n",
      "def test_weights():\n",
      "    kindel.weights(bwa_fns[0], relative=True)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# CLI\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# SAMPLE-SPECIFIC FUNCTIONAL REGRESSION\n",
      "['*', '*', '*', '*', '*', '*', '*', '*', '*', '*']\n",
      "# Lint as: python3\n",
      "# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "r\"\"\"Script to generate inputs/outputs exclusion lists for GradientTape.\n",
      "\n",
      "To use this script:\n",
      "\n",
      "bazel run tensorflow/python/eager:gradient_input_output_exclusions -- \\\n",
      "  $PWD/tensorflow/python/eager/pywrap_gradient_exclusions.cc\n",
      "\"\"\"\n",
      "\n",
      "from __future__ import absolute_import\n",
      "from __future__ import division\n",
      "from __future__ import print_function\n",
      "\n",
      "import argparse\n",
      "import sys\n",
      "\n",
      "import gast\n",
      "\n",
      "from tensorflow.python.autograph.pyct import anno\n",
      "from tensorflow.python.autograph.pyct import cfg\n",
      "from tensorflow.python.autograph.pyct import parser\n",
      "from tensorflow.python.autograph.pyct import qual_names\n",
      "from tensorflow.python.autograph.pyct import transformer\n",
      "from tensorflow.python.autograph.pyct.static_analysis import activity\n",
      "from tensorflow.python.autograph.pyct.static_analysis import liveness\n",
      "from tensorflow.python.autograph.pyct.static_analysis import reaching_fndefs\n",
      "from tensorflow.python.framework import op_def_registry\n",
      "from tensorflow.python.framework import ops\n",
      "\n",
      "_GENERATED_FILE_HEADER = \"\"\"/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "==============================================================================*/\n",
      "\n",
      "// Inputs/Outputs exclusion lists for GradientTape.\n",
      "//\n",
      "// This file is MACHINE GENERATED! Do not edit.\n",
      "// Generated by: tensorflow/python/eager/gen_gradient_input_output_exclusions.py\n",
      "\"\"\"\n",
      "\n",
      "_INCLUDES = \"\"\"\n",
      "#include \"tensorflow/python/eager/pywrap_gradient_exclusions.h\"\n",
      "\n",
      "#include \"absl/types/optional.h\"\n",
      "#include \"tensorflow/core/lib/gtl/flatmap.h\"\n",
      "#include \"tensorflow/core/lib/gtl/flatset.h\"\n",
      "\n",
      "using tensorflow::string;\n",
      "\n",
      "namespace {\n",
      "// Keep static data in a format that's easy to init statically.\n",
      "struct OpIndexInfo {\n",
      "  const char *op_name;\n",
      "  int num_indices;\n",
      "  std::array<int, 4> unused_indices;\n",
      "};\n",
      "\n",
      "// Helper function to initialize FlatMap<string,FlatSet> from OpIndexInfo.\n",
      "template <typename T>\n",
      "auto OpGradientInfoInit(const T &a) {\n",
      "  auto *m = new tensorflow::gtl::FlatMap<string, tensorflow::gtl::FlatSet<int>>;\n",
      "  for (const auto &item : a) {\n",
      "    m->emplace(string(item.op_name),\n",
      "               tensorflow::gtl::FlatSet<int>(\n",
      "                   item.unused_indices.begin(),\n",
      "                   item.unused_indices.begin() + item.num_indices));\n",
      "  }\n",
      "  return m;\n",
      "}\n",
      "}  // namespace\n",
      "\"\"\"\n",
      "\n",
      "_EXCLUDED_OPS = [\n",
      "    # Composite ops with custom gradient functions.\n",
      "    \"If\",\n",
      "    \"StatelessIf\",\n",
      "    \"While\",\n",
      "    \"StatelessWhile\",\n",
      "    \"Case\",\n",
      "\n",
      "    # TF Lite. These ops only appear in OSS.\n",
      "    # TODO(srbs): Find a better way to filter these out.\n",
      "    \"AudioMicrofrontend\",\n",
      "]\n",
      "\n",
      "\n",
      "class _SubscriptUseTracker(transformer.Base):\n",
      "  \"\"\"Track uses of composite names, excluding certain names when subscripted.\"\"\"\n",
      "\n",
      "  def __init__(self, ctx, exclude_when_subscripted):\n",
      "    super(_SubscriptUseTracker, self).__init__(ctx)\n",
      "    self.exclude = exclude_when_subscripted\n",
      "    self.reads = set()\n",
      "    self.complex_reads = set()\n",
      "\n",
      "  def visit_Attribute(self, node):\n",
      "    \"\"\"Visits attribute nodes in the AST.\"\"\"\n",
      "    if anno.hasanno(node, anno.Basic.QN):\n",
      "      qn = anno.getanno(node, anno.Basic.QN)\n",
      "      if isinstance(node.ctx, gast.Load):\n",
      "        self.reads.add(qn)\n",
      "    node = self.generic_visit(node)\n",
      "    return node\n",
      "\n",
      "  def visit_Subscript(self, node):\n",
      "    \"\"\"Visits nodes with subscript in the AST.\"\"\"\n",
      "    s = node.slice\n",
      "    if anno.hasanno(node, anno.Basic.QN):\n",
      "      qn = anno.getanno(node, anno.Basic.QN)\n",
      "      if isinstance(node.ctx, gast.Load):\n",
      "        self.reads.add(qn)\n",
      "    elif isinstance(s, (gast.Tuple, gast.Slice)):\n",
      "      if anno.hasanno(node.value, anno.Basic.QN):\n",
      "        self.complex_reads.add(anno.getanno(node.value, anno.Basic.QN))\n",
      "    value_qn = anno.getanno(node.value, anno.Basic.QN, None)\n",
      "    if value_qn in self.exclude:\n",
      "      node.value = self.generic_visit(node.value)\n",
      "    else:\n",
      "      node.value = self.visit(node.value)\n",
      "    node.slice = self.visit(s)\n",
      "    return node\n",
      "\n",
      "\n",
      "class _FunctionCallsTracker(transformer.Base):\n",
      "  \"\"\"Tracks any function calls made with a given first argument name.\"\"\"\n",
      "\n",
      "  def __init__(self, ctx, first_argument_name):\n",
      "    super(_FunctionCallsTracker, self).__init__(ctx)\n",
      "    self.first_argument_name = first_argument_name\n",
      "    self.calls = set()\n",
      "\n",
      "  def visit_Name(self, node):\n",
      "    node = self.generic_visit(node)\n",
      "    if isinstance(node.ctx, gast.Load) and node.id in self.ctx.info.namespace:\n",
      "      anno.setanno(node, \"static_value\", self.ctx.info.namespace[node.id])\n",
      "    return node\n",
      "\n",
      "  def visit_Attribute(self, node):\n",
      "    node = self.generic_visit(node)\n",
      "    parent_val = anno.getanno(node.value, \"static_value\", default=None)\n",
      "    if parent_val is not None:\n",
      "      if hasattr(parent_val, node.attr):\n",
      "        anno.setanno(node, \"static_value\", getattr(parent_val, node.attr))\n",
      "    return node\n",
      "\n",
      "  def visit_Call(self, node):\n",
      "    node = self.generic_visit(node)\n",
      "    if (node.args and anno.getanno(node.args[0], anno.Basic.QN,\n",
      "                                   None) == self.first_argument_name):\n",
      "      fn_object = anno.getanno(node.func, \"static_value\", None)\n",
      "      if fn_object is not None:\n",
      "        self.calls.add(fn_object)\n",
      "    return node\n",
      "\n",
      "\n",
      "_ALL = object()\n",
      "\n",
      "\n",
      "def _live_tensors(f, attr_name=\"inputs\"):\n",
      "  \"\"\"Returns the indices of the used inputs.\n",
      "\n",
      "  Note: This currently only handles direct index accesses e.g. op.inputs[1].\n",
      "  If the function has slicing or list comprehension on attr_name then returns\n",
      "  _ALL. This ensure that this is correct even if inefficient.\n",
      "\n",
      "  Args:\n",
      "    f: A grad function, taking the op as first argument.\n",
      "    attr_name: op attr to track. \"inputs\" or \"outputs\".\n",
      "\n",
      "  Returns:\n",
      "    Either one of:\n",
      "      * set of integers representing individual indices of inputs used\n",
      "      * the value _ALL, if indices are used but cannot be determined which\n",
      "      * empty set, if no inputs are used\n",
      "  \"\"\"\n",
      "  node, _ = parser.parse_entity(f, ())\n",
      "  entity_info = transformer.EntityInfo(\n",
      "      name=f.__name__,\n",
      "      source_code=None,\n",
      "      source_file=None,\n",
      "      future_features=(),\n",
      "      namespace=sys.modules[f.__module__].__dict__)\n",
      "  ctx = transformer.Context(entity_info, None, None)\n",
      "\n",
      "  graphs = cfg.build(node)\n",
      "  node = qual_names.resolve(node)\n",
      "  node = activity.resolve(node, ctx, None)\n",
      "  node = reaching_fndefs.resolve(node, ctx, graphs)\n",
      "  node = liveness.resolve(node, ctx, graphs)\n",
      "\n",
      "  op_arg_name = anno.getanno(node.args.args[0], anno.Basic.QN)\n",
      "  op_inputs_outputs_name = qual_names.QN(op_arg_name, attr=attr_name)\n",
      "\n",
      "  special_tracker = _SubscriptUseTracker(ctx, (op_inputs_outputs_name,))\n",
      "  node = special_tracker.visit(node)\n",
      "\n",
      "  live_vars_in = anno.getanno(node.body[0], anno.Static.LIVE_VARS_IN)\n",
      "  inputs_outputs_used_qns = set()\n",
      "  for v in special_tracker.complex_reads:\n",
      "    # Complicated patterns like op.inputs[:3]. Could be smarter about them\n",
      "    # if they matter much.\n",
      "    if v == op_inputs_outputs_name:\n",
      "      return _ALL\n",
      "  for v in live_vars_in:\n",
      "    if v in special_tracker.reads:\n",
      "      if (v.has_subscript() and v.parent == op_inputs_outputs_name):\n",
      "        inputs_outputs_used_qns.add(v)\n",
      "      elif v == op_inputs_outputs_name:\n",
      "        # When op.{attr_name} is used directly, assume all tensors are\n",
      "        # used for now. In that case, no point digging further.\n",
      "        # TODO(mdan): We can descend into tuple expansions.\n",
      "        return _ALL\n",
      "\n",
      "  function_calls_tracker = _FunctionCallsTracker(ctx, op_arg_name)\n",
      "  node = function_calls_tracker.visit(node)\n",
      "\n",
      "  input_output_indices = set()\n",
      "\n",
      "  for called_f in function_calls_tracker.calls:\n",
      "    child_indices = _live_tensors(called_f, attr_name=attr_name)\n",
      "    if child_indices is _ALL:\n",
      "      return _ALL\n",
      "    input_output_indices |= child_indices\n",
      "\n",
      "  for v in inputs_outputs_used_qns:\n",
      "    assert v.has_subscript()\n",
      "    _, subscript = v.qn\n",
      "    if not subscript.is_simple():\n",
      "      # Not a number, assuming it can be anything.\n",
      "      return _ALL\n",
      "    subscript_val, = subscript.qn\n",
      "    if (not isinstance(subscript_val, qual_names.Literal) and\n",
      "        not isinstance(subscript_val.value, int)):\n",
      "      # Not a number, assuming it can be anything.\n",
      "      return _ALL\n",
      "    input_output_indices.add(subscript_val.value)\n",
      "  return input_output_indices\n",
      "\n",
      "\n",
      "def _get_num_inputs_outputs(op_type):\n",
      "  \"\"\"Returns (num_inputs, num_outputs).\n",
      "\n",
      "  Args:\n",
      "    op_type: String. The type of the Operation. Used to lookup the op in the\n",
      "      registry.\n",
      "\n",
      "  Returns:\n",
      "    (num_inputs, num_outputs), for either num_inputs or num_outputs if the value\n",
      "    can't be statically inferred from the OpDef alone or of the OpDef lookup\n",
      "    fails, -1 is returned.\n",
      "  \"\"\"\n",
      "\n",
      "  def _is_list_arg(arg):\n",
      "    return arg.number_attr or arg.type_list_attr\n",
      "\n",
      "  def _count_args(arg_defs):\n",
      "    for arg in arg_defs:\n",
      "      if _is_list_arg(arg):\n",
      "        # Op has list type args which could be variable.\n",
      "        return -1\n",
      "    return len(arg_defs)\n",
      "\n",
      "  op_def = op_def_registry.get(op_type)\n",
      "  if not op_def:\n",
      "    return -1, -1\n",
      "  return _count_args(op_def.input_arg), _count_args(op_def.output_arg)\n",
      "\n",
      "\n",
      "def get_entries(attr_name):\n",
      "  \"\"\"Returns the dict of entries.\n",
      "\n",
      "  Each entry is of the form {op_name, {true|false, indices}}\n",
      "\n",
      "  true: All values are unused.\n",
      "  false: `indices` are the only unused indices.\n",
      "\n",
      "  Note: ops for which all values are used are not printed.\n",
      "\n",
      "  Args:\n",
      "    attr_name: inputs or outputs.\n",
      "\n",
      "  Returns:\n",
      "    A dict from op_type to formatted entry in the dict.\n",
      "  \"\"\"\n",
      "  assert attr_name in [\"inputs\", \"outputs\"]\n",
      "  entries = {}\n",
      "  for op_type in ops._gradient_registry.list():  # pylint: disable=protected-access\n",
      "    if op_type in _EXCLUDED_OPS:\n",
      "      continue\n",
      "    num_values = _get_num_inputs_outputs(op_type)[0 if attr_name ==\n",
      "                                                  \"inputs\" else 1]\n",
      "    gradient_fn = ops._gradient_registry.lookup(op_type)  # pylint: disable=protected-access\n",
      "    if gradient_fn is None:\n",
      "      # NotDifferentiable\n",
      "      if num_values != -1:\n",
      "        entries[op_type] = \"{\\\"%s\\\"},\" % op_type\n",
      "      continue\n",
      "    used_tensors = _live_tensors(gradient_fn, attr_name=attr_name)\n",
      "    if used_tensors is _ALL:\n",
      "      continue\n",
      "    elif not used_tensors:\n",
      "      entries[op_type] = \"{\\\"%s\\\"},\" % op_type\n",
      "    else:\n",
      "      all_tensors = set(range(num_values))\n",
      "      unused_tensors = all_tensors - used_tensors\n",
      "      if unused_tensors:\n",
      "        unused_tensor_list = sorted(list(unused_tensors))\n",
      "        entries[op_type] = \"{\\\"%s\\\", %d, {%s}},\" % (\n",
      "            op_type, len(unused_tensor_list), \", \".join(\n",
      "                str(i) for i in unused_tensor_list))\n",
      "  return entries\n",
      "\n",
      "\n",
      "def get_function(name, entries):\n",
      "  \"\"\"Generates lookup function with given name and lookup table entries.\"\"\"\n",
      "  contents = \"\"\"\n",
      "absl::optional<tensorflow::gtl::FlatSet<int>> {name}(\n",
      "    const tensorflow::string &op_name) {{\n",
      "  static std::array<OpIndexInfo, {count}> a = {{{{\n",
      "\"\"\".format(\n",
      "    name=name, count=len(entries) + 1)\n",
      "  contents += \"      \"\n",
      "  contents += \"\\n      \".join(entries[op_type] for op_type in sorted(entries))\n",
      "  contents += \"\\n      {\\\"VarHandleOp\\\"},\"\n",
      "  contents += \"\"\"\n",
      "  }};\n",
      "  static const auto &m = *OpGradientInfoInit(a);\n",
      "\n",
      "  auto it = m.find(op_name);\n",
      "  if (it != m.end()) {\n",
      "    return it->second;\n",
      "  }\n",
      "  return absl::nullopt;\n",
      "}\n",
      "\"\"\"\n",
      "  return contents\n",
      "\n",
      "\n",
      "def get_contents():\n",
      "  \"\"\"Returns contents for the generated file.\"\"\"\n",
      "  contents = \"\"\n",
      "  contents += _GENERATED_FILE_HEADER + _INCLUDES\n",
      "  contents += get_function(\"OpGradientUnusedInputIndices\",\n",
      "                           get_entries(\"inputs\"))\n",
      "  contents += get_function(\"OpGradientUnusedOutputIndices\",\n",
      "                           get_entries(\"outputs\"))\n",
      "  return contents\n",
      "\n",
      "\n",
      "def main(output_file):\n",
      "  with open(output_file, \"w\") as fp:\n",
      "    fp.write(get_contents())\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "  arg_parser = argparse.ArgumentParser()\n",
      "  arg_parser.add_argument(\"output\", metavar=\"O\", type=str, help=\"Output file.\")\n",
      "  args = arg_parser.parse_args()\n",
      "  main(args.output)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in dataframe['content']:\n",
    "    print(['*']*10)\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "378ecb15-a4fa-470b-8a19-3ea389cc041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deduplicated = dataframe.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4c510d98-4e68-4e84-8c50-ae1da7912831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(669, 5)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_deduplicated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4631d52a-e4d5-4f70-b736-26641fcd4563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157e1a52-6b7f-44be-9d3d-941521f7a0bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1c8228-217d-4d79-8e8b-d4eda2f22857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43d99f3-7ec2-4bdb-90ac-24a7a5c710aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
