{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a3bb1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "cwd = os.getcwd()  # get directory for storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe7dd9d",
   "metadata": {},
   "source": [
    "# This file automates the entire pipeline for assertion generation with chatgpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51bc35b",
   "metadata": {},
   "source": [
    "## Step 1) Get Asserted Code From Github\n",
    "\n",
    "### Step 1.1) Clean and process the code\n",
    "### Step 1.2) Extract Ground-Truth Assertions & Relevant Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "126af672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Running Query:\n",
      "SELECT f.repo_name, c.content\n",
      "FROM `bigquery-public-data.github_repos.files` AS f\n",
      "JOIN `bigquery-public-data.github_repos.contents` AS c\n",
      "ON f.id = c.id\n",
      "WHERE\n",
      "NOT c.binary\n",
      "AND f.path LIKE '%.py'\n",
      "AND REGEXP_CONTAINS(c.content, r'(?m)^\\s*assert ')\n",
      "LIMIT 10\n",
      "\n",
      "*Handling Duplicates...\n",
      "Duplicate Ratio =  0.9\n",
      "\n",
      "*Extracting Assertions\n",
      "Weird assertion found:\n",
      " [\"'allow_null'\", 'not', 'in', 'kwargs'] \n",
      " assert self.source != field_name, (\n",
      "assert 'allow_null' not in kwargs, '`allow_null` is not a valid option. Use `NullBooleanField` instead.'\n",
      "assert 'allow_null' not in kwargs, '`allow_null` is not a valid option.'\n",
      "\n",
      "Weird assertion found:\n",
      " [\"'allow_null'\", 'not', 'in', 'kwargs'] \n",
      " assert 'allow_null' not in kwargs, '`allow_null` is not a valid option. Use `NullBooleanField` instead.'\n",
      "assert 'allow_null' not in kwargs, '`allow_null` is not a valid option.'\n",
      "assert not isinstance(value, datetime.datetime), (\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_name</th>\n",
       "      <th>content</th>\n",
       "      <th>assertions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ghmajx/asuswrt-merlin</td>\n",
       "      <td>import sys, string, SambaParm\\nfrom smbparm im...</td>\n",
       "      <td>[[parm_table.has_key(, self.key, )]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trust-Code/connector-magento</td>\n",
       "      <td># -*- coding: utf-8 -*-\\n#####################...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>angelapper/edx-platform</td>\n",
       "      <td>\"\"\"\\nTest the create_random_users command line...</td>\n",
       "      <td>[[(self.num_users_start + users_to_create), ==...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AndreaCrotti/breaking-changes</td>\n",
       "      <td>import os\\nimport pytest\\n\\nfrom unittest impo...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sszlm/MissionPlanner</td>\n",
       "      <td>\"\"\"A parser for HTML and XHTML.\"\"\"\\r\\n\\r\\n# Th...</td>\n",
       "      <td>[[msg, ==, True], [0, ==, True], [rawdata[i:i+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WebSpider/SickRage</td>\n",
       "      <td># testing/exclusions.py\\n# Copyright (C) 2005-...</td>\n",
       "      <td>[[False, ==, True], [queries, ==, True]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sheppard/django-rest-framework</td>\n",
       "      <td>from __future__ import unicode_literals\\n\\nimp...</td>\n",
       "      <td>[[(read_only, ==, False], [write_only), ==, Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tkinz27/ansible</td>\n",
       "      <td># This code is part of Ansible, but is an inde...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sffjunkie/home-assistant</td>\n",
       "      <td>\"\"\"Test Home Assistant yaml loader.\"\"\"\\nimport...</td>\n",
       "      <td>[[doc['config'], ==, [\"simple\"], [doc['key'], ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        repo_name  \\\n",
       "0           ghmajx/asuswrt-merlin   \n",
       "2    Trust-Code/connector-magento   \n",
       "3         angelapper/edx-platform   \n",
       "4   AndreaCrotti/breaking-changes   \n",
       "5            sszlm/MissionPlanner   \n",
       "6              WebSpider/SickRage   \n",
       "7  sheppard/django-rest-framework   \n",
       "8                 tkinz27/ansible   \n",
       "9        sffjunkie/home-assistant   \n",
       "\n",
       "                                             content  \\\n",
       "0  import sys, string, SambaParm\\nfrom smbparm im...   \n",
       "2  # -*- coding: utf-8 -*-\\n#####################...   \n",
       "3  \"\"\"\\nTest the create_random_users command line...   \n",
       "4  import os\\nimport pytest\\n\\nfrom unittest impo...   \n",
       "5  \"\"\"A parser for HTML and XHTML.\"\"\"\\r\\n\\r\\n# Th...   \n",
       "6  # testing/exclusions.py\\n# Copyright (C) 2005-...   \n",
       "7  from __future__ import unicode_literals\\n\\nimp...   \n",
       "8  # This code is part of Ansible, but is an inde...   \n",
       "9  \"\"\"Test Home Assistant yaml loader.\"\"\"\\nimport...   \n",
       "\n",
       "                                          assertions  \n",
       "0               [[parm_table.has_key(, self.key, )]]  \n",
       "2                                                 []  \n",
       "3  [[(self.num_users_start + users_to_create), ==...  \n",
       "4                                                 []  \n",
       "5  [[msg, ==, True], [0, ==, True], [rawdata[i:i+...  \n",
       "6           [[False, ==, True], [queries, ==, True]]  \n",
       "7  [[(read_only, ==, False], [write_only), ==, Tr...  \n",
       "8                                                 []  \n",
       "9  [[doc['config'], ==, [\"simple\"], [doc['key'], ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import bigquery as bq\n",
    "\n",
    "def get_asserted_code(num=100000, ext=\"%.py\", verbose=True):\n",
    "    query_string = \"\"\"SELECT f.repo_name, c.content\n",
    "FROM `bigquery-public-data.github_repos.files` AS f\n",
    "JOIN `bigquery-public-data.github_repos.contents` AS c\n",
    "ON f.id = c.id\n",
    "WHERE\n",
    "NOT c.binary\n",
    "AND f.path LIKE '%.py'\n",
    "AND REGEXP_CONTAINS(c.content, r'(?m)^\\s*assert ')\n",
    "LIMIT \"\"\" + str(num)\n",
    "    \n",
    "    if isinstance(num, int):\n",
    "        secret_dir = \"Data/secret/\"\n",
    "        api_key = cwd + \"/\" + secret_dir + os.listdir(secret_dir)[0]\n",
    "        assert api_key[-5:] == \".json\"  # confirm that it was found\n",
    "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = api_key\n",
    "        query_string = query_string.replace(\"%.py\", ext)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"*Running Query:\")\n",
    "            print(query_string)\n",
    "            print()\n",
    "        client = bq.Client()\n",
    "        df = (\n",
    "            client.query(query_string)\n",
    "            .result()\n",
    "            .to_dataframe(\n",
    "                create_bqstorage_client=True,\n",
    "            )\n",
    "        )\n",
    "    elif isinstance(num, str):\n",
    "        # load data from file\n",
    "        df = pd.read_csv(num)\n",
    "        print(\"Found data at\", num)\n",
    "    else:\n",
    "        print(\"first param type undefined, must be string signifying directory of csv or\\\n",
    "               int signifying number of records to scrib from bigquery...\")\n",
    "        assert False\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"*Handling Duplicates...\")\n",
    "    init_len = len(df)\n",
    "    df.drop_duplicates(subset=[\"content\"], keep=\"first\", inplace=True)\n",
    "    if verbose:\n",
    "        print(\"Duplicate Ratio = \", (len(df)/init_len))\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"\\n*Extracting Assertions\")\n",
    "    df[\"assertions\"] = df[\"content\"].apply(lambda code: get_assertions(code, True, verbose))\n",
    "    return df\n",
    "\n",
    "\n",
    "conditionals = dict([[cond, i] for i, cond in enumerate([\"==\", \"!=\", \"<=\", \">=\", \"<\", \">\"])])\n",
    "compounding_statements = [\"and\"]\n",
    "bad_statements = [\"or\"]  # TODO: properly account for OR\n",
    "def get_assertions(func, is_split=True, verbose=True):\n",
    "    \"\"\"\n",
    "    Format: \"assert [expression], [return_string]\"\n",
    "    \n",
    "    Exceptions to Handle:\n",
    "    - 'in'/'not in' keyword\n",
    "    - boolean functions - ex. isinstance(var, type)\n",
    "    - separation of attributes - ex. len(var), var[i]\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    lines = []\n",
    "    for temp in func.split('\\n'):\n",
    "        if \"assert\" in temp:\n",
    "            bad_flag = False\n",
    "            for bad in bad_statements:\n",
    "                if bad in temp:\n",
    "                    bad_flag = True\n",
    "                    break\n",
    "            if not bad_flag:\n",
    "                lines.append(temp.strip())\n",
    "    # TODO: experiment with smaller content window for assertions\n",
    "    ind = 0\n",
    "    while ind < len(lines):\n",
    "        data = lines[ind].strip()\n",
    "        start = data.find('assert')\n",
    "        if start != -1:\n",
    "            # account for combination statements\n",
    "            for statement in compounding_statements:\n",
    "                add_statement = data.find(statement)\n",
    "                if add_statement != -1:\n",
    "                    extra_line = data[add_statement+len(statement):]\n",
    "                    lines.insert(ind+1, \"assert \"+extra_line)\n",
    "                    data = data[:add_statement].strip()\n",
    "            \n",
    "            com = data.find(',')   # parsing out return_string\n",
    "            if com != -1:\n",
    "                data = data[:com]\n",
    "\n",
    "            if is_split:\n",
    "                data = [var.strip() for var in data.split()]\n",
    "                if data[0] != \"assert\":\n",
    "                    print(\"something was found before the assertion in this line:\\n\", data)\n",
    "                    break\n",
    "                data = data[1:]\n",
    "                \n",
    "                condition = True  # assertion [variable] == condition by default\n",
    "                if data[0] == \"not\":  # accounting for not\n",
    "                    condition = False\n",
    "                    data = data[1:]\n",
    "                    \n",
    "                assert len(data) >= 1, \"empty assertion found?: \" + data\n",
    "                if len(data) == 1:  # adding == to simlify\n",
    "                    data = data + [\"==\", str(condition)]\n",
    "                \n",
    "                for i in range(len(data)):\n",
    "                    if data[i] == \"is\":  # simplifying is to ==\n",
    "                        data[i] = \"==\"\n",
    "                    if data[i] in conditionals.keys():  # com\n",
    "                        data = [' '.join(data[:i]), data[i], ' '.join(data[i+1:])]  # conditionals[data[i]]\n",
    "                        break\n",
    "            \n",
    "            if verbose and len(data) != 3:\n",
    "                print(\"Weird assertion found:\\n\", data, '\\n', '\\n'.join(lines[ind-1:ind+2]))\n",
    "                print()\n",
    "#             assert len(data) == 3, \"found conditional-less assertion:\\n\" + str(data) + '\\n' + str(lines[ind-1:ind+2])\n",
    "            else:\n",
    "                out.append(data)\n",
    "        ind += 1\n",
    "    return out\n",
    "\n",
    "# small test\n",
    "verilog_dir = cwd+\"/Data/BigQuery/VerilogAssertions-ALL.csv\"\n",
    "tester_df = get_asserted_code(get_asserted_code(10))\n",
    "tester_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9210d204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_name</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tqchen/tvm</td>\n",
       "      <td># Licensed to the Apache Software Foundation (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lujeni/ansible</td>\n",
       "      <td># (c) 2017 Red Hat Inc.\\n#\\n# This file is par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lukas-hetzenecker/home-assistant</td>\n",
       "      <td>\"\"\"The tests for the Pilight sensor platform.\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>schnoebe/fedora-mock</td>\n",
       "      <td>import fcntl\\nimport glob\\nimport grp\\nimport ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>samstav/fastfood</td>\n",
       "      <td># -*- coding: utf-8 -*-\\n# Copyright 2015 Rack...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33788</th>\n",
       "      <td>raphaelm/django-i18nfield</td>\n",
       "      <td>from i18nfield.admin import I18nModelAdmin\\nfr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33789</th>\n",
       "      <td>fniephaus/alfred-rworkflow</td>\n",
       "      <td># The MIT License (MIT)\\n#\\n# Copyright (c) 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33790</th>\n",
       "      <td>bgris/ODL_bgris</td>\n",
       "      <td># -*- coding: utf-8 -*-\\r\\n#\\r\\n# Copyright © ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33791</th>\n",
       "      <td>chrsrds/scikit-learn</td>\n",
       "      <td>\"\"\"\\nTesting for the base module (sklearn.ense...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33792</th>\n",
       "      <td>perimosocordiae/bigO</td>\n",
       "      <td>'''Symbolic manipulation of Big-O complexities...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33793 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              repo_name  \\\n",
       "0                            tqchen/tvm   \n",
       "1                        Lujeni/ansible   \n",
       "2      lukas-hetzenecker/home-assistant   \n",
       "3                  schnoebe/fedora-mock   \n",
       "4                      samstav/fastfood   \n",
       "...                                 ...   \n",
       "33788         raphaelm/django-i18nfield   \n",
       "33789        fniephaus/alfred-rworkflow   \n",
       "33790                   bgris/ODL_bgris   \n",
       "33791              chrsrds/scikit-learn   \n",
       "33792              perimosocordiae/bigO   \n",
       "\n",
       "                                                 content  \n",
       "0      # Licensed to the Apache Software Foundation (...  \n",
       "1      # (c) 2017 Red Hat Inc.\\n#\\n# This file is par...  \n",
       "2      \"\"\"The tests for the Pilight sensor platform.\"...  \n",
       "3      import fcntl\\nimport glob\\nimport grp\\nimport ...  \n",
       "4      # -*- coding: utf-8 -*-\\n# Copyright 2015 Rack...  \n",
       "...                                                  ...  \n",
       "33788  from i18nfield.admin import I18nModelAdmin\\nfr...  \n",
       "33789  # The MIT License (MIT)\\n#\\n# Copyright (c) 20...  \n",
       "33790  # -*- coding: utf-8 -*-\\r\\n#\\r\\n# Copyright © ...  \n",
       "33791  \"\"\"\\nTesting for the base module (sklearn.ense...  \n",
       "33792  '''Symbolic manipulation of Big-O complexities...  \n",
       "\n",
       "[33793 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = pd.read_csv(cwd+\"/Data/BigQuery/PythonAssertions100k.csv\")\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "289b22a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Licensed to the Apache Software Foundation (ASF) under one\n",
      "# or more contributor license agreements.  See the NOTICE file\n",
      "# distributed with this work for additional information\n",
      "# regarding copyright ownership.  The ASF licenses this file\n",
      "# to you under the Apache License, Version 2.0 (the\n",
      "# \"License\"); you may not use this file except in compliance\n",
      "# with the License.  You may obtain a copy of the License at\n",
      "#\n",
      "#   http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing,\n",
      "# software distributed under the License is distributed on an\n",
      "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
      "# KIND, either express or implied.  See the License for the\n",
      "# specific language governing permissions and limitations\n",
      "# under the License.\n",
      "# pylint: disable=invalid-name, missing-docstring, no-else-return\n",
      "\"\"\"Unit tests for the Relay VM serialization and deserialization.\"\"\"\n",
      "import pytest\n",
      "import numpy as np\n",
      "\n",
      "import tvm\n",
      "from tvm.runtime import vm as _vm\n",
      "from tvm.relay import vm as rly_vm\n",
      "from tvm import relay\n",
      "\n",
      "from tvm.relay.scope_builder import ScopeBuilder\n",
      "from tvm.relay import transform\n",
      "from tvm.relay.prelude import Prelude\n",
      "from tvm.contrib import util\n",
      "from tvm.relay import testing\n",
      "\n",
      "\n",
      "def create_exec(f, target=\"llvm\", params=None):\n",
      "    if isinstance(f, relay.Expr):\n",
      "        mod = tvm.IRModule()\n",
      "        mod[\"main\"] = f\n",
      "        executable = rly_vm.compile(mod, target=target, params=params)\n",
      "        return executable\n",
      "    else:\n",
      "        assert isinstance(f, tvm.IRModule), \"expected mod as tvm.IRModule\"\n",
      "        executable = rly_vm.compile(f, target=target, params=params)\n",
      "        return executable\n",
      "\n",
      "\n",
      "def get_serialized_output(mod, *data, params=None, target=\"llvm\", ctx=tvm.cpu()):\n",
      "    exe = create_exec(mod, target, params=params)\n",
      "    code, lib = exe.save()\n",
      "    des_exec = _vm.Executable.load_exec(code, lib)\n",
      "    des_vm = _vm.VirtualMachine(des_exec, ctx)\n",
      "    result = des_vm.run(*data)\n",
      "    return result\n",
      "\n",
      "\n",
      "def run_network(mod, params, dtype=\"float32\"):\n",
      "    def get_vm_output(mod, data, params, target, ctx, dtype=\"float32\"):\n",
      "        ex = relay.create_executor(\"vm\", mod=mod, ctx=ctx)\n",
      "        result = ex.evaluate()(data, **params)\n",
      "        return result.asnumpy().astype(dtype)\n",
      "\n",
      "    data_shape = [int(x) for x in mod[\"main\"].checked_type.arg_types[0].shape]\n",
      "    data = np.random.uniform(size=data_shape).astype(dtype)\n",
      "    target = \"llvm\"\n",
      "    ctx = tvm.cpu(0)\n",
      "\n",
      "    tvm_out = get_vm_output(mod, tvm.nd.array(data.astype(dtype)), params, target, ctx, dtype)\n",
      "    vm_out = get_serialized_output(\n",
      "        mod, tvm.nd.array(data.astype(dtype)), params=params, target=target, ctx=ctx\n",
      "    )\n",
      "    tvm.testing.assert_allclose(vm_out.asnumpy().astype(dtype), tvm_out, rtol=1e-5, atol=1e-5)\n",
      "\n",
      "\n",
      "def test_serializer():\n",
      "    mod = tvm.IRModule({})\n",
      "    a = relay.const(1.0, \"float32\")\n",
      "    x = relay.var(\"x\", shape=(10, 10), dtype=\"float32\")\n",
      "    f1 = relay.Function([x], x + a)\n",
      "    glb_f1 = relay.GlobalVar(\"f1\")\n",
      "    mod[glb_f1] = f1\n",
      "\n",
      "    # TODO(@jroesch): look into optimizing away the need to do this\n",
      "    mod = transform.InferType()(mod)\n",
      "\n",
      "    b = relay.const(2.0, \"float32\")\n",
      "    y = relay.var(\"y\", shape=(10, 10), dtype=\"float32\")\n",
      "    f2 = relay.Function([y], y - b)\n",
      "    glb_f2 = relay.GlobalVar(\"f2\")\n",
      "    mod[glb_f2] = f2\n",
      "\n",
      "    # TODO(@jroesch): look into optimizing away the need to do this\n",
      "    mod = transform.InferType()(mod)\n",
      "\n",
      "    x1 = relay.var(\"x1\", shape=(10, 10), dtype=\"float32\")\n",
      "    y1 = relay.var(\"y1\", shape=(10, 10), dtype=\"float32\")\n",
      "    main = relay.Function([x1, y1], glb_f1(x1) * glb_f2(y1))\n",
      "    mod[\"main\"] = main\n",
      "\n",
      "    exe = create_exec(mod)\n",
      "\n",
      "    glbs = exe.globals\n",
      "    assert len(glbs) == 3\n",
      "    assert \"f1\" in glbs\n",
      "    assert \"f2\" in glbs\n",
      "    assert \"main\" in glbs\n",
      "\n",
      "    prim_ops = exe.primitive_ops\n",
      "    assert any(item.startswith(\"fused_add\") for item in prim_ops)\n",
      "    assert any(item.startswith(\"fused_subtract\") for item in prim_ops)\n",
      "    assert any(item.startswith(\"fused_multiply\") for item in prim_ops)\n",
      "\n",
      "    code = exe.bytecode\n",
      "    assert \"main(x1, y1)\" in code\n",
      "    assert \"f1(x)\" in code\n",
      "    assert \"f2(y)\" in code\n",
      "\n",
      "    code, lib = exe.save()\n",
      "    assert isinstance(code, bytearray)\n",
      "    assert isinstance(lib, tvm.runtime.Module)\n",
      "\n",
      "\n",
      "def test_save_load():\n",
      "    x = relay.var(\"x\", shape=(10, 10))\n",
      "    f = relay.Function([x], x + x)\n",
      "    x_data = np.random.rand(10, 10).astype(\"float32\")\n",
      "\n",
      "    # serialize.\n",
      "    vm = create_exec(f)\n",
      "    code, lib = vm.save()\n",
      "    assert isinstance(code, bytearray)\n",
      "\n",
      "    # save and load the code and lib file.\n",
      "    tmp = util.tempdir()\n",
      "    path_lib = tmp.relpath(\"lib.so\")\n",
      "    lib.export_library(path_lib)\n",
      "    with open(tmp.relpath(\"code.ro\"), \"wb\") as fo:\n",
      "        fo.write(code)\n",
      "\n",
      "    loaded_lib = tvm.runtime.load_module(path_lib)\n",
      "    loaded_code = bytearray(open(tmp.relpath(\"code.ro\"), \"rb\").read())\n",
      "\n",
      "    # deserialize.\n",
      "    des_exec = _vm.Executable.load_exec(loaded_code, loaded_lib)\n",
      "    des_vm = _vm.VirtualMachine(des_exec, tvm.cpu())\n",
      "\n",
      "    res = des_vm.run(x_data)\n",
      "    tvm.testing.assert_allclose(res.asnumpy(), x_data + x_data)\n",
      "\n",
      "\n",
      "def test_const():\n",
      "    c = relay.const(1.0, \"float32\")\n",
      "    x = relay.var(\"x\", shape=(10, 10), dtype=\"float32\")\n",
      "    f = relay.Function([x], x + c)\n",
      "    x_data = np.random.rand(10, 10).astype(\"float32\")\n",
      "    res = get_serialized_output(f, x_data)\n",
      "    tvm.testing.assert_allclose(res.asnumpy(), x_data + 1)\n",
      "\n",
      "\n",
      "def test_if():\n",
      "    x = relay.var(\"x\", shape=(10, 10))\n",
      "    y = relay.var(\"y\", shape=(10, 10))\n",
      "    equal = relay.op.equal(x, y)\n",
      "    equal = relay.op.nn.batch_flatten(equal)\n",
      "    f = relay.Function([x, y], relay.If(relay.op.min(equal, axis=[0, 1]), x, y))\n",
      "    x_data = np.random.rand(10, 10).astype(\"float32\")\n",
      "    y_data = np.random.rand(10, 10).astype(\"float32\")\n",
      "\n",
      "    # same\n",
      "    res = get_serialized_output(f, x_data, x_data)\n",
      "    tvm.testing.assert_allclose(res.asnumpy(), x_data)\n",
      "\n",
      "    # diff\n",
      "    res = get_serialized_output(f, x_data, y_data)\n",
      "    tvm.testing.assert_allclose(res.asnumpy(), y_data)\n",
      "\n",
      "\n",
      "def test_loop():\n",
      "    mod = tvm.IRModule({})\n",
      "    sum_up = relay.GlobalVar(\"sum_up\")\n",
      "    i = relay.var(\"i\", shape=[], dtype=\"int32\")\n",
      "    accum = relay.var(\"accum\", shape=[], dtype=\"int32\")\n",
      "    sb = ScopeBuilder()\n",
      "    with sb.if_scope(relay.equal(i, relay.const(0, \"int32\"))):\n",
      "        sb.ret(accum)\n",
      "    with sb.else_scope():\n",
      "        one_less = relay.subtract(i, relay.const(1, \"int32\"))\n",
      "        new_accum = relay.add(accum, i)\n",
      "        sb.ret(relay.Call(sum_up, [one_less, new_accum]))\n",
      "    func = relay.Function([i, accum], sb.get())\n",
      "    mod[sum_up] = func\n",
      "    mod = transform.InferType()(mod)\n",
      "    loop_bound = 0\n",
      "    i_data = np.array(loop_bound, dtype=\"int32\")\n",
      "    accum_data = np.array(0, dtype=\"int32\")\n",
      "    iarg = relay.var(\"i\", shape=[], dtype=\"int32\")\n",
      "    aarg = relay.var(\"accum\", shape=[], dtype=\"int32\")\n",
      "    mod[\"main\"] = relay.Function([iarg, aarg], sum_up(iarg, aarg))\n",
      "\n",
      "    result = get_serialized_output(mod, i_data, accum_data)\n",
      "    tvm.testing.assert_allclose(result.asnumpy(), sum(range(1, loop_bound + 1)))\n",
      "\n",
      "\n",
      "def test_tuple():\n",
      "    ttype = relay.TupleType([relay.TensorType((1,)), relay.TensorType((10,))])\n",
      "    tup = relay.var(\"tup\", type_annotation=ttype)\n",
      "    f = relay.Function([tup], relay.TupleGetItem(tup, 1))\n",
      "    i_data = np.random.rand(41).astype(\"float32\")\n",
      "    j_data = np.random.rand(10).astype(\"float32\")\n",
      "\n",
      "    result = get_serialized_output(f, (i_data, j_data))\n",
      "    tvm.testing.assert_allclose(result.asnumpy(), j_data)\n",
      "\n",
      "\n",
      "def test_adt_list():\n",
      "    mod = tvm.IRModule()\n",
      "    p = Prelude(mod)\n",
      "    _, cons, nil = mod.get_type(\"List\")\n",
      "    l1 = cons(relay.const(1), nil())\n",
      "    l21 = cons(relay.const(2), l1)\n",
      "    l321 = cons(relay.const(3), l21)\n",
      "\n",
      "    f = relay.Function([], l321)\n",
      "    mod[\"main\"] = f\n",
      "\n",
      "    result = get_serialized_output(mod)\n",
      "    assert len(result) == 2\n",
      "    assert len(result[1]) == 2\n",
      "    assert len(result[1][1]) == 2\n",
      "    res = []\n",
      "    res.append(result[0].asnumpy().tolist())\n",
      "    res.append(result[1][0].asnumpy().tolist())\n",
      "    res.append(result[1][1][0].asnumpy().tolist())\n",
      "    tvm.testing.assert_allclose(res, np.array([3, 2, 1]))\n",
      "\n",
      "\n",
      "def test_adt_compose():\n",
      "    mod = tvm.IRModule()\n",
      "    p = Prelude(mod)\n",
      "\n",
      "    compose = mod.get_global_var(\"compose\")\n",
      "\n",
      "    # add_one = fun x -> x + 1\n",
      "    sb = relay.ScopeBuilder()\n",
      "    x = relay.var(\"x\", \"float32\")\n",
      "    x1 = sb.let(\"x1\", x)\n",
      "    xplusone = x1 + relay.const(1.0, \"float32\")\n",
      "    sb.ret(xplusone)\n",
      "    body = sb.get()\n",
      "    add_one = relay.GlobalVar(\"add_one\")\n",
      "    add_one_func = relay.Function([x], body)\n",
      "\n",
      "    # add_two = compose(add_one, add_one)\n",
      "    sb = relay.ScopeBuilder()\n",
      "    y = relay.var(\"y\", \"float32\")\n",
      "    add_two_func = sb.let(\"add_two\", compose(add_one_func, add_one_func))\n",
      "    add_two_res = add_two_func(y)\n",
      "    sb.ret(add_two_res)\n",
      "    add_two_body = sb.get()\n",
      "\n",
      "    mod[add_one] = add_one_func\n",
      "\n",
      "    f = relay.Function([y], add_two_body)\n",
      "    mod[\"main\"] = f\n",
      "\n",
      "    x_data = np.array(np.random.rand()).astype(\"float32\")\n",
      "    result = get_serialized_output(mod, x_data)\n",
      "    tvm.testing.assert_allclose(result.asnumpy(), x_data + 2.0)\n",
      "\n",
      "\n",
      "def test_closure():\n",
      "    x = relay.var(\"x\", shape=())\n",
      "    y = relay.var(\"y\", shape=())\n",
      "    f = relay.Function([x], x + y)\n",
      "    ff = relay.Function([y], f)\n",
      "    clo = ff(relay.const(1.0))\n",
      "    main = clo(relay.const(2.0))\n",
      "\n",
      "    res = get_serialized_output(main)\n",
      "    tvm.testing.assert_allclose(res.asnumpy(), 3.0)\n",
      "\n",
      "\n",
      "def test_synthetic():\n",
      "    mod, params = testing.synthetic.get_workload()\n",
      "    run_network(mod, params)\n",
      "\n",
      "\n",
      "def test_mobilenet():\n",
      "    mod, params = testing.mobilenet.get_workload(batch_size=1)\n",
      "    run_network(mod, params)\n",
      "\n",
      "\n",
      "def test_vm_shape_of():\n",
      "    x = relay.var(\"x\", shape=(relay.Any(), relay.Any(), relay.Any()), dtype=\"float32\")\n",
      "    relu_x = relay.nn.relu(x)\n",
      "    data = np.random.uniform(size=(2, 3, 4)).astype(\"float32\")\n",
      "    args = [data]\n",
      "\n",
      "    newshape_var = relay.var(\"newshape\", shape=(2,), dtype=\"int64\")\n",
      "    args.append(np.array((1, -1), dtype=\"int64\"))\n",
      "    main = relay.Function([x, newshape_var], relay.reshape(relu_x, newshape=newshape_var))\n",
      "\n",
      "    res = get_serialized_output(main, *args).asnumpy()\n",
      "    tvm.testing.assert_allclose(res.flatten(), data.flatten())\n",
      "\n",
      "\n",
      "def test_dynamic_bcast():\n",
      "    dtype = \"float32\"\n",
      "    x = relay.var(\"x\", shape=(relay.Any(), 2), dtype=dtype)\n",
      "    y = relay.var(\"y\", shape=(3, 2), dtype=dtype)\n",
      "    mod = tvm.IRModule()\n",
      "    mod[\"main\"] = relay.Function([x, y], relay.add(x, y))\n",
      "    x_data = np.random.uniform(size=(1, 2)).astype(dtype)\n",
      "    y_data = np.random.uniform(size=(3, 2)).astype(dtype)\n",
      "    res_np = np.add(x_data, y_data)\n",
      "    for target, ctx in testing.enabled_targets():\n",
      "        res = get_serialized_output(mod, *(x_data, y_data), target=target, ctx=ctx)\n",
      "        tvm.testing.assert_allclose(res.asnumpy(), res_np)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    pytest.main([__file__])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(temp[\"content\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a415702d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_name</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18-341/Router</td>\n",
       "      <td>`default_nettype none\\n`include \"RouterPkg.pkg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>swallat/yosys</td>\n",
       "      <td>module top (\\n  input clk, rst,\\n  output reg ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xuwenyihust/MapReduce_NoC</td>\n",
       "      <td>/******************FIFO_MUX*******************...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TheClams/SystemVerilog</td>\n",
       "      <td>module assertions_test #(parameter SIZE = pa_t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mda-ut/Tempest</td>\n",
       "      <td>// (C) 2001-2013 Altera Corporation. All right...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>litex-hub/pythondata-cpu-blackparrot</td>\n",
       "      <td>\\n`include \"bp_common_defines.svh\"\\n`include \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>litex-hub/pythondata-cpu-blackparrot</td>\n",
       "      <td>\\n`include \"bp_common_defines.svh\"\\n`include \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>litex-hub/pythondata-cpu-blackparrot</td>\n",
       "      <td>/**\\n *  Name:\\n *    bp_lce_req.v\\n *\\n *  De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>litex-hub/pythondata-cpu-blackparrot</td>\n",
       "      <td>\\n`include \"bp_common_defines.svh\"\\n`include \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>litex-hub/pythondata-cpu-blackparrot</td>\n",
       "      <td>/**\\n *  bp_nonsynth_nbf_loader.v\\n *\\n */\\n\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>446 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                repo_name  \\\n",
       "0                           18-341/Router   \n",
       "1                           swallat/yosys   \n",
       "2               xuwenyihust/MapReduce_NoC   \n",
       "3                  TheClams/SystemVerilog   \n",
       "4                          mda-ut/Tempest   \n",
       "..                                    ...   \n",
       "441  litex-hub/pythondata-cpu-blackparrot   \n",
       "442  litex-hub/pythondata-cpu-blackparrot   \n",
       "443  litex-hub/pythondata-cpu-blackparrot   \n",
       "444  litex-hub/pythondata-cpu-blackparrot   \n",
       "445  litex-hub/pythondata-cpu-blackparrot   \n",
       "\n",
       "                                               content  \n",
       "0    `default_nettype none\\n`include \"RouterPkg.pkg...  \n",
       "1    module top (\\n  input clk, rst,\\n  output reg ...  \n",
       "2    /******************FIFO_MUX*******************...  \n",
       "3    module assertions_test #(parameter SIZE = pa_t...  \n",
       "4    // (C) 2001-2013 Altera Corporation. All right...  \n",
       "..                                                 ...  \n",
       "441  \\n`include \"bp_common_defines.svh\"\\n`include \"...  \n",
       "442  \\n`include \"bp_common_defines.svh\"\\n`include \"...  \n",
       "443  /**\\n *  Name:\\n *    bp_lce_req.v\\n *\\n *  De...  \n",
       "444  \\n`include \"bp_common_defines.svh\"\\n`include \"...  \n",
       "445  /**\\n *  bp_nonsynth_nbf_loader.v\\n *\\n */\\n\\n...  \n",
       "\n",
       "[446 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = pd.read_csv(cwd+\"/Data/BigQuery/VerilogAssertions-ALL.csv\")\n",
    "temp  # example verilog assertion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9d8683",
   "metadata": {},
   "source": [
    "## Step 2) Generate LLM Prompt & Query a GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aeaaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(asserted_code, verbose=True):\n",
    "    ...\n",
    "    \n",
    "    \n",
    "banned_vars = ['', '*', 'self']\n",
    "def get_variables(func, verbose=False):\n",
    "    out = []\n",
    "    for line in func.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if \"def \" in line:  # add params if its a function\n",
    "            start = line.find('(')\n",
    "            end = line.find(')')\n",
    "            for new_param in line[start+1:end].split(','):\n",
    "                default = new_param.find(\"=\")\n",
    "                if default != -1:\n",
    "                    new_param = new_param[:default]\n",
    "                new_param = new_param.strip()\n",
    "                if new_param not in out and new_param not in banned_vars:\n",
    "                    if verbose:\n",
    "                        print(\"*Found  {\", new_param, \"}  at:\\n\", line, '\\n')\n",
    "                    out.append(new_param)\n",
    "        else: # add variables if equals operation\n",
    "            find_var = line.find(' = ')\n",
    "            if find_var != -1:\n",
    "                new_var = line[:find_var].strip()\n",
    "                \n",
    "                if ',' in new_var: # handle tuple equalities edge case (ex: a, b, c = fn_output())\n",
    "                    var_list = [tuple_var.strip() for tuple_var in new_var.split(',')]\n",
    "                else:\n",
    "                    var_list = [new_var]\n",
    "                for new_var in var_list:\n",
    "                    if new_var not in out and new_var not in banned_vars:\n",
    "                        if verbose:\n",
    "                            print(\"**Found  {\", new_var, \"}  at:\\n\", line, '\\n')\n",
    "                        out.append(new_var)\n",
    "            # TODO: handle indexing\n",
    "    return out\n",
    "\n",
    "# out = get_variables(df.sample()[\"content\"].iloc[0])\n",
    "get_vars = lambda code: get_variables(code)\n",
    "df[\"variables\"] = df[\"content\"].apply(get_vars)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd866c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# querying\n",
    "import openai\n",
    "import altair as alt\n",
    "import json\n",
    "from vega_datasets import data\n",
    "\n",
    "def run_gpt4(messages):\n",
    "    OPENAI_API_KEY = \"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\" # os.environ['OPENAI_API_KEY']\n",
    "    openai.api_key = OPENAI_API_KEY\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "# TODO: add coding language versatility\n",
    "def gpt_oneshot(input_prompt, directive=\"You are a helpful bot that adds assertions to pieces of Python code.\", verbose=False):\n",
    "    message_hist = [{\"role\": \"system\", \"content\": directive},\n",
    "                    {\"role\": \"user\", \"content\": input_prompt}]  # init\n",
    "    response = run_gpt4(message_hist)\n",
    "    if verbose:\n",
    "        print(\"chat_gpt: \", response, '\\n')\n",
    "#     message_hist.append({\"role\": \"system\", \"content\": response})\n",
    "    return response\n",
    "\n",
    "print(\"\\n\\n\", gpt_oneshot(\"what do you do?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73927da6",
   "metadata": {},
   "source": [
    "## Step 3) Parse & Evaluate GPT's Response\n",
    "\n",
    "### Step 3.1) Restore the assertion(s) generated to code and evaluate\n",
    "> Metrics of evaluation, does it run? does it add to the code? is it ground-truth-like? human evaluator rank? gpt evaluator rank?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a956fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
