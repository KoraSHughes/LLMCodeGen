{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34a91b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "cwd = os.getcwd()  # get directory for storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffb7521",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.rea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fd02ca",
   "metadata": {},
   "source": [
    "# This file automates the entire pipeline for assertion generation with chatgpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcee349",
   "metadata": {},
   "source": [
    "## Step 1) Get Asserted Code From Github\n",
    "\n",
    "### Step 1.1) Clean and process the code\n",
    "### Step 1.2) Extract Ground-Truth Assertions & Relevant Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e819b72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Running Query:\n",
      "SELECT f.repo_name, c.content\n",
      "FROM `bigquery-public-data.github_repos.files` AS f\n",
      "JOIN `bigquery-public-data.github_repos.contents` AS c\n",
      "ON f.id = c.id\n",
      "WHERE\n",
      "NOT c.binary\n",
      "AND f.path LIKE '%.py'\n",
      "AND REGEXP_CONTAINS(c.content, r'(?m)^\\s*assert ')\n",
      "LIMIT 10\n",
      "\n",
      "*Handling Duplicates...\n",
      "Duplicate Ratio =  0.9\n",
      "\n",
      "*Extracting Assertions\n",
      "Weird assertion found:\n",
      " [\"'allow_null'\", 'not', 'in', 'kwargs'] \n",
      " assert self.source != field_name, (\n",
      "assert 'allow_null' not in kwargs, '`allow_null` is not a valid option. Use `NullBooleanField` instead.'\n",
      "assert 'allow_null' not in kwargs, '`allow_null` is not a valid option.'\n",
      "\n",
      "Weird assertion found:\n",
      " [\"'allow_null'\", 'not', 'in', 'kwargs'] \n",
      " assert 'allow_null' not in kwargs, '`allow_null` is not a valid option. Use `NullBooleanField` instead.'\n",
      "assert 'allow_null' not in kwargs, '`allow_null` is not a valid option.'\n",
      "assert not isinstance(value, datetime.datetime), (\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_name</th>\n",
       "      <th>content</th>\n",
       "      <th>assertions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ghmajx/asuswrt-merlin</td>\n",
       "      <td>import sys, string, SambaParm\\nfrom smbparm im...</td>\n",
       "      <td>[[parm_table.has_key(, self.key, )]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trust-Code/connector-magento</td>\n",
       "      <td># -*- coding: utf-8 -*-\\n#####################...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>angelapper/edx-platform</td>\n",
       "      <td>\"\"\"\\nTest the create_random_users command line...</td>\n",
       "      <td>[[(self.num_users_start + users_to_create), ==...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AndreaCrotti/breaking-changes</td>\n",
       "      <td>import os\\nimport pytest\\n\\nfrom unittest impo...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sszlm/MissionPlanner</td>\n",
       "      <td>\"\"\"A parser for HTML and XHTML.\"\"\"\\r\\n\\r\\n# Th...</td>\n",
       "      <td>[[msg, ==, True], [0, ==, True], [rawdata[i:i+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WebSpider/SickRage</td>\n",
       "      <td># testing/exclusions.py\\n# Copyright (C) 2005-...</td>\n",
       "      <td>[[False, ==, True], [queries, ==, True]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sheppard/django-rest-framework</td>\n",
       "      <td>from __future__ import unicode_literals\\n\\nimp...</td>\n",
       "      <td>[[(read_only, ==, False], [write_only), ==, Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tkinz27/ansible</td>\n",
       "      <td># This code is part of Ansible, but is an inde...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sffjunkie/home-assistant</td>\n",
       "      <td>\"\"\"Test Home Assistant yaml loader.\"\"\"\\nimport...</td>\n",
       "      <td>[[doc['config'], ==, [\"simple\"], [doc['key'], ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        repo_name  \\\n",
       "0           ghmajx/asuswrt-merlin   \n",
       "2    Trust-Code/connector-magento   \n",
       "3         angelapper/edx-platform   \n",
       "4   AndreaCrotti/breaking-changes   \n",
       "5            sszlm/MissionPlanner   \n",
       "6              WebSpider/SickRage   \n",
       "7  sheppard/django-rest-framework   \n",
       "8                 tkinz27/ansible   \n",
       "9        sffjunkie/home-assistant   \n",
       "\n",
       "                                             content  \\\n",
       "0  import sys, string, SambaParm\\nfrom smbparm im...   \n",
       "2  # -*- coding: utf-8 -*-\\n#####################...   \n",
       "3  \"\"\"\\nTest the create_random_users command line...   \n",
       "4  import os\\nimport pytest\\n\\nfrom unittest impo...   \n",
       "5  \"\"\"A parser for HTML and XHTML.\"\"\"\\r\\n\\r\\n# Th...   \n",
       "6  # testing/exclusions.py\\n# Copyright (C) 2005-...   \n",
       "7  from __future__ import unicode_literals\\n\\nimp...   \n",
       "8  # This code is part of Ansible, but is an inde...   \n",
       "9  \"\"\"Test Home Assistant yaml loader.\"\"\"\\nimport...   \n",
       "\n",
       "                                          assertions  \n",
       "0               [[parm_table.has_key(, self.key, )]]  \n",
       "2                                                 []  \n",
       "3  [[(self.num_users_start + users_to_create), ==...  \n",
       "4                                                 []  \n",
       "5  [[msg, ==, True], [0, ==, True], [rawdata[i:i+...  \n",
       "6           [[False, ==, True], [queries, ==, True]]  \n",
       "7  [[(read_only, ==, False], [write_only), ==, Tr...  \n",
       "8                                                 []  \n",
       "9  [[doc['config'], ==, [\"simple\"], [doc['key'], ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import bigquery as bq\n",
    "\n",
    "def get_asserted_code(num=100000, ext=\"%.py\", verbose=True):\n",
    "    query_string = \"\"\"SELECT f.repo_name, c.content\n",
    "FROM `bigquery-public-data.github_repos.files` AS f\n",
    "JOIN `bigquery-public-data.github_repos.contents` AS c\n",
    "ON f.id = c.id\n",
    "WHERE\n",
    "NOT c.binary\n",
    "AND f.path LIKE '%.py'\n",
    "AND REGEXP_CONTAINS(c.content, r'(?m)^\\s*assert ')\n",
    "LIMIT \"\"\" + str(num)\n",
    "    \n",
    "    if isinstance(num, int):\n",
    "        secret_dir = \"Data/secret/\"\n",
    "        api_key = cwd + \"/\" + secret_dir + os.listdir(secret_dir)[0]\n",
    "        assert api_key[-5:] == \".json\"  # confirm that it was found\n",
    "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = api_key\n",
    "        query_string = query_string.replace(\"%.py\", ext)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"*Running Query:\")\n",
    "            print(query_string)\n",
    "            print()\n",
    "        client = bq.Client()\n",
    "        df = (\n",
    "            client.query(query_string)\n",
    "            .result()\n",
    "            .to_dataframe(\n",
    "                create_bqstorage_client=True,\n",
    "            )\n",
    "        )\n",
    "    elif isinstance(num, str):\n",
    "        # load data from file\n",
    "        df = pd.read_csv(num)\n",
    "        print(\"Found data at\", num)\n",
    "    else:\n",
    "        print(\"first param type undefined, must be string signifying directory of csv or\\\n",
    "               int signifying number of records to scrib from bigquery...\")\n",
    "        assert False\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"*Handling Duplicates...\")\n",
    "    init_len = len(df)\n",
    "    df.drop_duplicates(subset=[\"content\"], keep=\"first\", inplace=True)\n",
    "    if verbose:\n",
    "        print(\"Duplicate Ratio = \", (len(df)/init_len))\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"\\n*Extracting Assertions\")\n",
    "    df[\"assertions\"] = df[\"content\"].apply(lambda code: get_assertions(code, True, verbose))\n",
    "    return df\n",
    "\n",
    "\n",
    "conditionals = dict([[cond, i] for i, cond in enumerate([\"==\", \"!=\", \"<=\", \">=\", \"<\", \">\"])])\n",
    "compounding_statements = [\"and\"]\n",
    "bad_statements = [\"or\"]  # TODO: properly account for OR\n",
    "def get_assertions(func, is_split=True, verbose=True):\n",
    "    \"\"\"\n",
    "    Format: \"assert [expression], [return_string]\"\n",
    "    \n",
    "    Exceptions to Handle:\n",
    "    - 'in'/'not in' keyword\n",
    "    - boolean functions - ex. isinstance(var, type)\n",
    "    - separation of attributes - ex. len(var), var[i]\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    lines = []\n",
    "    for temp in func.split('\\n'):\n",
    "        if \"assert\" in temp:\n",
    "            bad_flag = False\n",
    "            for bad in bad_statements:\n",
    "                if bad in temp:\n",
    "                    bad_flag = True\n",
    "                    break\n",
    "            if not bad_flag:\n",
    "                lines.append(temp.strip())\n",
    "    # TODO: experiment with smaller content window for assertions\n",
    "    ind = 0\n",
    "    while ind < len(lines):\n",
    "        data = lines[ind].strip()\n",
    "        start = data.find('assert')\n",
    "        if start != -1:\n",
    "            # account for combination statements\n",
    "            for statement in compounding_statements:\n",
    "                add_statement = data.find(statement)\n",
    "                if add_statement != -1:\n",
    "                    extra_line = data[add_statement+len(statement):]\n",
    "                    lines.insert(ind+1, \"assert \"+extra_line)\n",
    "                    data = data[:add_statement].strip()\n",
    "            \n",
    "            com = data.find(',')   # parsing out return_string\n",
    "            if com != -1:\n",
    "                data = data[:com]\n",
    "\n",
    "            if is_split:\n",
    "                data = [var.strip() for var in data.split()]\n",
    "                if data[0] != \"assert\":\n",
    "                    print(\"something was found before the assertion in this line:\\n\", data)\n",
    "                    break\n",
    "                data = data[1:]\n",
    "                \n",
    "                condition = True  # assertion [variable] == condition by default\n",
    "                if data[0] == \"not\":  # accounting for not\n",
    "                    condition = False\n",
    "                    data = data[1:]\n",
    "                    \n",
    "                assert len(data) >= 1, \"empty assertion found?: \" + data\n",
    "                if len(data) == 1:  # adding == to simlify\n",
    "                    data = data + [\"==\", str(condition)]\n",
    "                \n",
    "                for i in range(len(data)):\n",
    "                    if data[i] == \"is\":  # simplifying is to ==\n",
    "                        data[i] = \"==\"\n",
    "                    if data[i] in conditionals.keys():  # com\n",
    "                        data = [' '.join(data[:i]), data[i], ' '.join(data[i+1:])]  # conditionals[data[i]]\n",
    "                        break\n",
    "            \n",
    "            if verbose and len(data) != 3:\n",
    "                print(\"Weird assertion found:\\n\", data, '\\n', '\\n'.join(lines[ind-1:ind+2]))\n",
    "                print()\n",
    "#             assert len(data) == 3, \"found conditional-less assertion:\\n\" + str(data) + '\\n' + str(lines[ind-1:ind+2])\n",
    "            else:\n",
    "                out.append(data)\n",
    "        ind += 1\n",
    "    return out\n",
    "\n",
    "# small test\n",
    "tester_df = get_asserted_code(cwd+\"/Data/BigQuery/VerilogAssertions-ALL.csv\")  # get_asserted_code(10)\n",
    "tester_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b7968b",
   "metadata": {},
   "source": [
    "## Step 2) Generate LLM Prompt & Query a GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d853e143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(asserted_code, verbose=True):\n",
    "    ...\n",
    "    \n",
    "    \n",
    "banned_vars = ['', '*', 'self']\n",
    "def get_variables(func, verbose=False):\n",
    "    out = []\n",
    "    for line in func.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if \"def \" in line:  # add params if its a function\n",
    "            start = line.find('(')\n",
    "            end = line.find(')')\n",
    "            for new_param in line[start+1:end].split(','):\n",
    "                default = new_param.find(\"=\")\n",
    "                if default != -1:\n",
    "                    new_param = new_param[:default]\n",
    "                new_param = new_param.strip()\n",
    "                if new_param not in out and new_param not in banned_vars:\n",
    "                    if verbose:\n",
    "                        print(\"*Found  {\", new_param, \"}  at:\\n\", line, '\\n')\n",
    "                    out.append(new_param)\n",
    "        else: # add variables if equals operation\n",
    "            find_var = line.find(' = ')\n",
    "            if find_var != -1:\n",
    "                new_var = line[:find_var].strip()\n",
    "                \n",
    "                if ',' in new_var: # handle tuple equalities edge case (ex: a, b, c = fn_output())\n",
    "                    var_list = [tuple_var.strip() for tuple_var in new_var.split(',')]\n",
    "                else:\n",
    "                    var_list = [new_var]\n",
    "                for new_var in var_list:\n",
    "                    if new_var not in out and new_var not in banned_vars:\n",
    "                        if verbose:\n",
    "                            print(\"**Found  {\", new_var, \"}  at:\\n\", line, '\\n')\n",
    "                        out.append(new_var)\n",
    "            # TODO: handle indexing\n",
    "    return out\n",
    "\n",
    "# out = get_variables(df.sample()[\"content\"].iloc[0])\n",
    "get_vars = lambda code: get_variables(code)\n",
    "df[\"variables\"] = df[\"content\"].apply(get_vars)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fd2c92",
   "metadata": {},
   "source": [
    "## Step 3) Parse & Evaluate GPT's Response\n",
    "\n",
    "### Step 3.1) Restore the assertion(s) generated to code and evaluate\n",
    "> Metrics of evaluation, does it run? does it add to the code? is it ground-truth-like? human evaluator rank? gpt evaluator rank?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2a936c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
